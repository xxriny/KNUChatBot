# í†µí•©ì½”ë“œ.py
import os
import csv
import time
import re
import random
from datetime import datetime
from urllib.parse import urljoin
import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

HEADERS = {"User-Agent": "Mozilla/5.0"}
SAVE_FOLDER = "í†µí•©_images"
CSV_FILE = "ë©”ì¸20000ì—¬ê°œ.csv"
os.makedirs(SAVE_FOLDER, exist_ok=True)

# ìš”ì²­ ì„¸ì…˜ ì„¤ì • (ì¬ì‹œë„ ë° ë°±ì˜¤í”„ ì ìš©)
session = requests.Session()
retry = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
adapter = HTTPAdapter(max_retries=retry)
session.mount("http://", adapter)
session.mount("https://", adapter)

all_data = []

# ë§¨ ìœ„ì— ì¶”ê°€
batch_size = 100  # ì¤‘ê°„ ì €ì¥ ì£¼ê¸°
record_count = 0  # ëˆ„ì  ì¹´ìš´íŠ¸


def sanitize_filename(name):
    return "".join(c if c.isalnum() or c in (' ', '.', '_') else '_' for c in name)

def save_image(img_url, folder, prefix, idx, original_name="image.jpg"):
    try:
        os.makedirs(folder, exist_ok=True)
        ext = os.path.splitext(original_name)[1]
        if not ext or len(ext) > 5:
            ext = '.jpg'
        filename = sanitize_filename(f"{prefix}_{idx}{ext}")
        filepath = os.path.join(folder, filename)

        res = session.get(img_url, headers=HEADERS, timeout=10)
        time.sleep(random.uniform(0.5, 1.2))
        if res.status_code == 200 and len(res.content) > 1024:
            with open(filepath, "wb") as f:
                f.write(res.content)
            return filepath.replace("\\", "/")
        else:
            print(f"âš ï¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ë„ˆë¬´ ì‘ìŒ: {img_url}")
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {img_url} ({e})")
    return None

def clean_html_keep_table(raw_html):
    soup = BeautifulSoup(raw_html, 'html.parser')
    output = ''
    for table in soup.find_all('table'):
        output += extract_table_text(table) + '\n'
        table.decompose()
    for br in soup.find_all("br"):
        br.replace_with("\n")
    for elem in soup.find_all(['p', 'div', 'span']):
        text = elem.get_text(strip=True, separator="\n")
        if text:
            output += text + '\n'
    return output.strip()

def extract_table_text(table):
    rows = table.find_all('tr')
    return '\n'.join(
        ' | '.join(col.get_text(strip=True) for col in row.find_all(['td', 'th']) if col.get_text(strip=True))
        for row in rows if row.find_all(['td', 'th'])
    )

def extract_written_date(soup):
    info_div = soup.select_one("div.bbs_right.bbs_count")
    if info_div:
        for span in info_div.find_all("span"):
            span_text = span.get_text(strip=True)
            if span_text.startswith("ì‘ì„±ì¼"):
                strong = span.find("strong")
                raw_date = strong.get_text(strip=True) if strong else span_text.replace("ì‘ì„±ì¼", "").strip()
                try:
                    dt = datetime.strptime(raw_date[:19], "%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„ %Sì´ˆ")
                    date = dt.strftime("%Y.%m.%d")
                except ValueError:
                    date = raw_date

    text = soup.get_text(" ", strip=True)
    match = re.search(r'20\d{2}[.\-/ë…„\s]+[01]?\d[.\-/ì›”\s]+[0-3]?\d[ì¼\s]*', text)
    if match:
        raw = match.group().replace(" ", "").replace("ë…„", ".").replace("ì›”", ".").replace("ì¼", "")
        return raw.strip(".")
    return "(ì‘ì„±ì¼ ì—†ìŒ)"

def extract_img_links_from_filebox(soup, base_url):
    img_links = []
    for a in soup.select("div.b-file-box a.file-down-btn"):
        name = a.text.strip()
        href = a.get("href", "")
        if not href:
            continue
        full_link = base_url + "/padm/life/notice-department.do" + href if href.startswith('?') else href
        if name.lower().endswith(('.png', '.jpg', '.jpeg')):
            img_links.append(full_link)
    return img_links

# ì´í›„ crawl_mainpage, crawl_library, crawl_administration, crawl_engineering í•¨ìˆ˜ ë‚´ì˜
# ëª¨ë“  requests.get() í˜¸ì¶œì„ session.get()ìœ¼ë¡œ ë³€ê²½ + time.sleep(random.uniform(0.5, 1.2)) ì¶”ê°€
# ì˜ˆì‹œ:
# r = session.get(detail_url, headers=HEADERS)
# time.sleep(random.uniform(0.5, 1.2))

# ë§ˆì§€ë§‰ ë¶€ë¶„ì˜ with open(...) êµ¬ë¬¸ì€ ê·¸ëŒ€ë¡œ ë‘ë˜, ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë¡œê·¸ë„ ì¶”ê°€í•˜ë©´ ì¢‹ìŒ


def crawl_mainpage():
    print("\nğŸ“‚ [ë©”ì¸í˜ì´ì§€] ì‹œì‘")

    BASE_URL = "https://www.kangwon.ac.kr"
    PATH_PREFIX = "/www"
    HEADERS_LOCAL = {"User-Agent": "Mozilla/5.0"}

    categories = [
        {"name": "ê³µì§€ì‚¬í•­", "bbsNo": "81", "key": "277", "last_page": 1459},
        {"name": "í–‰ì‚¬ì•ˆë‚´", "bbsNo": "38", "key": "279", "last_page": 242},
        {"name": "ê³µëª¨ëª¨ì§‘", "bbsNo": "345", "key": "1959", "last_page": 312},
        {"name": "ì¥í•™ê²Œì‹œíŒ", "bbsNo": "34", "key": "232", "last_page": 238},
    ]

    visited_links = set()

    for cat in categories:
        for page in range(1, 101):
            list_url = f"{BASE_URL}{PATH_PREFIX}/selectBbsNttList.do?bbsNo={cat['bbsNo']}&pageUnit=10&key={cat['key']}&pageIndex={page}"
            res = session.get(list_url, headers=HEADERS_LOCAL)
            time.sleep(random.uniform(0.5, 1.2))
            soup = BeautifulSoup(res.text, 'html.parser')
            rows = soup.select("tbody tr")

            for row in rows:
                if row.select_one(".notice"):
                    continue
                a_tag = row.select_one("td.subject a")
                if not a_tag:
                    continue
                title = a_tag.text.strip()
                href = a_tag.get("href", "")

                if "fnSelectBbsNttView" in href:
                    match = re.search(r"fnSelectBbsNttView\('(\d+)',\s*'(\d+)',\s*'(\d+)'\)", href)
                    if not match:
                        continue
                    bbs_no, ntt_no, key_param = match.groups()
                    detail_url = f"{BASE_URL}{PATH_PREFIX}/selectBbsNttView.do?bbsNo={bbs_no}&nttNo={ntt_no}&key={key_param}"
                else:
                    detail_url = urljoin(f"{BASE_URL}{PATH_PREFIX}/", href)

                if detail_url in visited_links:
                    continue
                visited_links.add(detail_url)

                try:
                    r = session.get(detail_url, headers=HEADERS_LOCAL)
                    time.sleep(random.uniform(0.5, 1.2))
                    s = BeautifulSoup(r.text, 'html.parser')

                    content_div = s.select_one("div#bbs_ntt_cn_con") or s.select_one("td.bbs_content") or s.select_one("div.bbs_content")
                    content = content_div.get_text("\n", strip=True) if content_div else "(ë³¸ë¬¸ ì—†ìŒ)"
                    date = extract_written_date(s)

                    img_tags = []
                    if content_div:
                        img_tags += content_div.select("img")
                    photo_div = s.select_one("div.photo_area")
                    if photo_div:
                        img_tags += photo_div.select("img")

                    images = [urljoin(detail_url, img.get("src")) for img in img_tags if img.get("src") and img.get("src").lower().endswith((".png", ".jpg", ".jpeg", ".gif"))]
                    img_files = [save_image(link, os.path.join(SAVE_FOLDER, "main"), cat['bbsNo'], i) for i, link in enumerate(images)]
                    img_files = list(filter(None, img_files))

                    all_data.append({
                        "ì œëª©": title,
                        "ì‘ì„±ì¼": date,
                        "ë³¸ë¬¸ë‚´ìš©": content,
                        "ë§í¬": detail_url,
                        "ì‚¬ì§„": ";".join(img_files)
                        
                    })
                    print(f"ğŸ“„ [{cat['name']}] {page}p - {title[:25]}")

                except Exception as e:
                    print(f"âŒ ìƒì„¸ í˜ì´ì§€ ì‹¤íŒ¨: {title[:30]} ({e})")







# ========================================
# ğŸŸ¦ 2. ë„ì„œê´€ í¬ë¡¤ëŸ¬
# ========================================
def crawl_library():
    print("\nğŸ“‚ [ë„ì„œê´€] ì‹œì‘")
    base_url = "https://library.kangwon.ac.kr"
    list_api = f"{base_url}/pyxis-api/1/bulletin-boards/24/bulletins"
    detail_api = f"{base_url}/pyxis-api/1/bulletins/24/{{id}}"

    per_page = 10
    total_count = None
    seen_ids = set()
    page = 0  # offset ë°©ì‹ ì‚¬ìš© ì‹œ page ëŒ€ì‹  offset index

    while True:
        params = {"offset": page * per_page, "max": 0}  # ğŸ” í•µì‹¬ ë³€ê²½
        res = session.get(list_api, headers=HEADERS, params=params)
        time.sleep(random.uniform(0.5, 1.2))

        data = res.json().get("data", {})
        list_data = data.get("list", [])

        if total_count is None:
            total_count = data.get("totalCount", 0)
            print(f"  ğŸ“Œ ì „ì²´ ê³µì§€ ìˆ˜: {total_count}ê°œ")

        if not list_data:
            print("  ğŸ”š ë„ì„œê´€ ê³µì§€ ë")
            break

        for item in list_data:
            id_ = item['id']
            if id_ in seen_ids:
                continue
            seen_ids.add(id_)

            title = item['title']
            detail_url = f"{base_url}/community/bulletin/notice/{id_}"

            detail = session.get(detail_api.format(id=id_), headers=HEADERS).json().get("data", {})
            time.sleep(random.uniform(0.5, 1.2))
            raw_date = detail.get("dateCreated", "ì‘ì„±ì¼ ì—†ìŒ")[:10]
            date = raw_date.replace("-", ".")
            html = detail.get("content", "")
            soup = BeautifulSoup(html, "html.parser")
            content = soup.get_text("\n", strip=True)

            images = [urljoin(base_url, img['src']) for img in soup.find_all("img")
                      if img.get("src") and "/pyxis-api/attachments/" in img['src']]
            img_files = [save_image(link, os.path.join(SAVE_FOLDER, "library"), id_, i)
                         for i, link in enumerate(images)]

            all_data.append({
                "ì œëª©": title,
                "ì‘ì„±ì¼": date,
                "ë³¸ë¬¸ë‚´ìš©": content,
                "ë§í¬": detail_url,
                "ì‚¬ì§„": ";".join(filter(None, img_files))
            })

            print(f"ğŸ“„ [ë„ì„œê´€] offset={page * per_page} - {title[:25]}")

        if (page + 1) * per_page >= total_count:
            break
        page += 1




# ========================================
# ğŸŸ¦ 3. í–‰ì •í•™ê³¼ í¬ë¡¤ëŸ¬
# ========================================
def crawl_administration():
    print("\nğŸ“‚ [í–‰ì •í•™ê³¼] ì‹œì‘")
    base_url = "https://padm.kangwon.ac.kr"

    for offset in range(0, 8000, 10):
        url = f"{base_url}/padm/life/notice-department.do?article.offset={offset}"
        try:
            res = requests.get(url, headers=HEADERS)
            time.sleep(random.uniform(0.5, 1.2))  # âœ… ëª©ë¡ í˜ì´ì§€ ìš”ì²­ í›„ ëŒ€ê¸°
            soup = BeautifulSoup(res.text, 'html.parser')
        except Exception as e:
            print(f"âŒ ëª©ë¡ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ (offset={offset}): {e}")
            continue

        for idx, row in enumerate(soup.select("td.b-td-left.b-td-title")):
            a_tag = row.select_one("a")
            if not a_tag:
                continue

            title = a_tag.text.strip()
            if "ê³µì§€" in title:
                continue  # [ê³µì§€]ê°€ í¬í•¨ëœ ì œëª©ì€ í¬ë¡¤ë§ ì œì™¸
            relative = a_tag.get("href", "")
            if '?' in relative:
                detail_link = f"{base_url}/padm/life/notice-department.do{relative[relative.find('?'):]}"
            else:
                detail_link = f"{base_url}/padm/life/notice-department.do"
            print(f"ğŸ“„ [í–‰ì •í•™ê³¼] {offset}~ - {title[:25]}")  # âœ… ê°„ì†Œí™”ëœ ë¡œê·¸

            try:
                r = requests.get(detail_link, headers=HEADERS)
                time.sleep(random.uniform(0.5, 1.2))  # âœ… ìƒì„¸ í˜ì´ì§€ ìš”ì²­ í›„ ëŒ€ê¸°
                s = BeautifulSoup(r.text, 'html.parser')

                # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
                content_div = s.select_one("div.b-content-box div.fr-view") or s.select_one("div.b-content-box")
                content = clean_html_keep_table(str(content_div)) if content_div else "(ë³¸ë¬¸ ì—†ìŒ)"

                # ì‘ì„±ì¼ ì¶”ì¶œ
                date_tag = s.select_one("li.b-date-box span:nth-of-type(2)")
                date = date_tag.text.strip() if date_tag else "(ì‘ì„±ì¼ ì—†ìŒ)"

                # ì²¨ë¶€ ì´ë¯¸ì§€ ë§í¬ ì¶”ì¶œ
                img_links = []
                for a in s.select("div.b-file-box a.file-down-btn"):
                    name = a.text.strip()
                    file_href = a.get("href", "")
                    if not file_href:
                        continue
                    full_link = (
                        base_url + "/padm/life/notice-department.do" + file_href
                        if file_href.startswith('?') else file_href
                    )
                    if name.lower().endswith(('.png', '.jpg', '.jpeg')):
                        img_links.append(full_link)

                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                img_files = [
                    save_image(link, os.path.join(SAVE_FOLDER, "admin"), offset + idx, i)
                    for i, link in enumerate(img_links)
                ]
                img_files = list(filter(None, img_files))
                img_field = ";".join(img_files).strip()

                # ë°ì´í„° ì €ì¥
                all_data.append({
                    "ì œëª©": title,
                    "ì‘ì„±ì¼": date,
                    "ë³¸ë¬¸ë‚´ìš©": content,
                    "ë§í¬": detail_link,
                    "ì‚¬ì§„": img_field
                })

            except Exception as e:
                print(f"âŒ ìƒì„¸ í˜ì´ì§€ ì‹¤íŒ¨: {title[:30]} ({e})")




# ========================================
# ğŸŸ¦ 4. ê³µí•™êµìœ¡í˜ì‹ ì„¼í„° í¬ë¡¤ëŸ¬
# ========================================
def crawl_engineering():
    print("\nğŸ“‚ [ê³µí•™êµìœ¡í˜ì‹ ì„¼í„°] ì‹œì‘")
    base_url = "https://icee.kangwon.ac.kr"

    for page in range(1, 19):
        url = f"{base_url}/index.php?mt=page&mp=5_1&mm=oxbbs&oxid=1&cpage={page}"
        try:
            res = requests.get(url, headers=HEADERS)
            time.sleep(random.uniform(0.5, 1.2))
            soup = BeautifulSoup(res.text, 'html.parser')
        except Exception as e:
            print(f"âŒ ëª©ë¡ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ (page={page}): {e}")
            continue

        for row in soup.select("table.bbs_list tbody tr"):
            a_tag = row.select_one("td.tit a")
            if not a_tag:
                continue

            title = a_tag.text.strip()
            href = urljoin(base_url, a_tag['href'])
            raw_date = row.select_one("td.dt").text.strip().replace("-", ".")

            try:
                r = requests.get(href, headers=HEADERS)
                time.sleep(random.uniform(0.5, 1.2))
                s = BeautifulSoup(r.text, 'html.parser')
                content_div = s.select_one("div.view_cont") or s.select_one("div.note")
                content = content_div.get_text("\n", strip=True) if content_div else "(ë³¸ë¬¸ ì—†ìŒ)"

                imgs = content_div.find_all("img") if content_div else []
                img_files = []
                for i, img in enumerate(imgs):
                    src = img.get("src")
                    if src and not src.startswith("data:image"):
                        img_files.append(save_image(
                            urljoin(href, src),
                            os.path.join(SAVE_FOLDER, "engineering"),
                            page, i
                        ))

                all_data.append({
                    "ì œëª©": title,
                    "ì‘ì„±ì¼": raw_date,
                    "ë³¸ë¬¸ë‚´ìš©": content,
                    "ë§í¬": href,
                    "ì‚¬ì§„": ";".join(filter(None, img_files))
                })

                print(f"ğŸ“„ [ê³µí•™êµìœ¡í˜ì‹ ì„¼í„°] {page}p - {title[:25]}")

            except Exception as e:
                print(f"âŒ ìƒì„¸ í˜ì´ì§€ ì‹¤íŒ¨: {title[:30]} ({e})")



# ========================================
# ğŸŸ¦ ì‹¤í–‰
# ========================================
if __name__ == "__main__":
    crawl_mainpage()
    

    # CSV ì €ì¥
    with open(CSV_FILE, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=["ì œëª©", "ì‘ì„±ì¼", "ë³¸ë¬¸ë‚´ìš©", "ë§í¬", "ì‚¬ì§„"])
        writer.writeheader()
        writer.writerows(all_data)
    print(f"\nâœ… í†µí•© CSV ì €ì¥ ì™„ë£Œ: {CSV_FILE}")
