# í†µí•© í¬ë¡¤ë§ ì½”ë“œ (ê°•ì›ëŒ€ ì „ ê¸°ê´€ + ì „ í•™ê³¼)
import os
import csv
import time
import re
import random
import unicodedata
from datetime import datetime
from urllib.parse import urljoin
import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from tqdm import tqdm
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

HEADERS = {"User-Agent": "Mozilla/5.0"}
SAVE_FOLDER = "ê°•ì›ëŒ€_ê³µì§€_ì´ë¯¸ì§€"
CSV_FILE = "ê°•ì›ëŒ€_ê³µì§€_í†µí•©.csv"
os.makedirs(SAVE_FOLDER, exist_ok=True)

session = requests.Session()
retry = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
adapter = HTTPAdapter(max_retries=retry)
session.mount("http://", adapter)
session.mount("https://", adapter)

all_data = []
existing_notice_keys = set()

def sanitize_filename(name):
    return re.sub(r'[^\w\s_.-]', '_', name).strip()

def save_image(img_url, folder, prefix, idx, original_name="image.jpg"):
    try:
        os.makedirs(folder, exist_ok=True)
        ext = os.path.splitext(original_name)[1]
        if not ext or len(ext) > 5:
            ext = '.jpg'
        filename = sanitize_filename(f"{prefix}_{idx}{ext}")
        filepath = os.path.join(folder, filename)

        res = session.get(img_url, headers=HEADERS, timeout=10)
        time.sleep(random.uniform(0.5, 1.2))
        if res.status_code == 200 and len(res.content) > 1024:
            with open(filepath, "wb") as f:
                f.write(res.content)
            return filepath.replace("\\", "/")
    except:
        pass
    return None

def clean_html_keep_table(raw_html):
    soup = BeautifulSoup(raw_html, 'html.parser')
    output = ''
    for table in soup.find_all('table'):
        output += extract_table_text(table) + '\n'
        table.decompose()
    for br in soup.find_all("br"):
        br.replace_with("\n")
    for elem in soup.find_all(['p', 'div', 'span']):
        text = elem.get_text(strip=True, separator="\n")
        if text:
            output += text + '\n'
    return output.strip()

def extract_table_text(table):
    rows = table.find_all('tr')
    return '\n'.join(
        ' | '.join(col.get_text(strip=True) for col in row.find_all(['td', 'th']) if col.get_text(strip=True))
        for row in rows if row.find_all(['td', 'th'])
    )

def extract_written_date(soup):
    text = soup.get_text(" ", strip=True)
    match = re.search(r'20\d{2}[.\-/ë…„\s]+[01]?\d[.\-/ì›”\s]+[0-3]?\d[ì¼\s]*', text)
    if match:
        raw = match.group().replace(" ", "").replace("ë…„", ".").replace("ì›”", ".").replace("ì¼", "")
        return raw.strip(".")
    return "(ì‘ì„±ì¼ ì—†ìŒ)"

def generate_notice_key(title, date):
    while re.search(r'(\[[^\]]*\]|\{[^\}]*\}|<[^>]*>)', title):
        title = re.sub(r'(\[[^\]]*\]|\{[^\}]*\}|<[^>]*>)', '', title)
    title = re.sub(r'[^ê°€-í£a-z0-9()]', '', unicodedata.normalize('NFKC', title.lower()))
    date = date.replace('.', '').replace('-', '').replace('/', '').replace(' ', '').lower()
    return f"{title}_{date}"

def add_notice_if_not_duplicate(title, date, content, link, images):
    key = generate_notice_key(title, date)
    if key not in existing_notice_keys:
        existing_notice_keys.add(key)
        all_data.append({
            "ì œëª©": title,
            "ì‘ì„±ì¼": date,
            "ë³¸ë¬¸ë‚´ìš©": content,
            "ë§í¬": link,
            "ì‚¬ì§„": images
        })
    else:
        print(f"â›” ì¤‘ë³µìœ¼ë¡œ ê±´ë„ˆëœ€: {title[:40]} ({date})")


def get_soup(url):
    try:
        response = session.get(url, headers=HEADERS, timeout=15)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        return BeautifulSoup(response.text, 'html.parser')
    except:
        return None

def extract_notice_board_urls(college_pages_list):
    from urllib.parse import urljoin

    department_boards_dict = {}
    base_wwwk_url = "https://wwwk.kangwon.ac.kr"

    for college in tqdm(college_pages_list, desc="ë‹¨ê³¼ëŒ€í•™ í˜ì´ì§€ ì²˜ë¦¬ ì¤‘"):
        soup = get_soup(college['url'])
        if not soup:
            continue
        blocks = soup.select("div.box.temp_titbox")
        for block in blocks:
            dept = block.select_one("h4.h0")
            link = block.select_one("ul.shortcut li:last-child a")
            if dept and link:
                name = dept.text.strip().split('\n')[0]

                # âœ… ìˆ˜ë™ ë§¤í•‘ì— ìˆëŠ” í•™ê³¼ëŠ” ìë™ ë“±ë¡í•˜ì§€ ì•ŠìŒ
                if name in manual_board_mapping:
                    continue

                href = link.get("href")
                if href:
                    url = urljoin(base_wwwk_url, href)
                    url = url.replace("wwwk.kangwon.ac.kr/wwwk.kangwon.ac.kr", "wwwk.kangwon.ac.kr")
                    department_boards_dict[name] = url
        time.sleep(0.1)

    # âœ… ìˆ˜ë™ ë§¤í•‘ í•™ê³¼ ì¶”ê°€
    for dept_name, manual_url in manual_board_mapping.items():
        department_boards_dict[dept_name] = manual_url

    return department_boards_dict



def crawl_all_departments(board_dict, max_page=None):
    def append_offset(url, offset):
        parsed = urlparse(url)
        query = parse_qs(parsed.query)
        query['article.offset'] = [str(offset)]
        new_query = urlencode(query, doseq=True)
        return urlunparse(parsed._replace(query=new_query))

    for dept, url in tqdm(board_dict.items(), desc="í•™ê³¼ë³„ ì§„í–‰"):
        print(f"\nğŸ“˜ [{dept}] ì‹œì‘: {url}")
        count = 0
        page = 0
        while True:
            if max_page is not None and page >= max_page:
                print(f"  ğŸ”š ìµœëŒ€ í˜ì´ì§€({max_page}) ë„ë‹¬ â†’ ì¢…ë£Œ")
                break

            page_url = append_offset(url, page * 10)
            soup = get_soup(page_url)
            if not soup:
                print(f"  âŒ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ (page={page + 1})")
                break

            rows = soup.select("tbody tr")
            if not rows:
                print("  ğŸ”š ë” ì´ìƒ í–‰ ì—†ìŒ â†’ ì¢…ë£Œ")
                break

            has_general_post = False

            for row in rows:
                no_td = row.select_one("td")
                is_notice = no_td and "ê³µì§€" in no_td.text

                a_tag = row.select_one("a")
                if not a_tag:
                    continue

                title = a_tag.get_text(strip=True)
                href = urljoin(url, a_tag.get("href"))

                try:
                    detail_soup = get_soup(href)
                    if not detail_soup:
                        continue
                    content_div = detail_soup.select_one("div.fr-view") or detail_soup.select_one("div.view-content")
                    content = clean_html_keep_table(str(content_div)) if content_div else "(ë³¸ë¬¸ ì—†ìŒ)"
                    date = extract_written_date(detail_soup)

                    imgs = []

                    img_tags = detail_soup.find_all("img")
                    for i, img in enumerate(img_tags):
                        src = img.get("src") or img.get("data-src") or img.get("srcset", "").split()[0]
                        if not src or src.startswith("data:image"):
                            continue

                        src_lower = src.lower()
                        if any(keyword in src_lower for keyword in ['logo', 'favicon', '/common/', '/img/', '/images/', '/static/', 'knu_ci']):
                            continue
                        if not any(path in src_lower for path in ['/upload/', '/editor/', '/bbs/', '/board/', '/notice/', '/file/', '/attach/']):
                            continue

                        full_img_url = urljoin(href, src)
                        saved = save_image(full_img_url, os.path.join(SAVE_FOLDER, "college_depts", dept), dept, i)
                        if saved:
                            imgs.append(saved)

                    for i, a in enumerate(detail_soup.select("a.file-down-btn")):
                        href_file = a.get("href", "")
                        name = a.get_text(strip=True)
                        if href_file and name.lower().endswith(('.jpg', '.jpeg', '.png')):
                            full_img_url = urljoin(href, href_file)
                            saved = save_image(full_img_url, os.path.join(SAVE_FOLDER, "college_depts", dept), dept, i + 100)
                            if saved:
                                imgs.append(saved)

                    add_notice_if_not_duplicate(title, date, content, href, ";".join(filter(None, imgs)))
                    print(f"    ğŸ“„ [{page + 1}p] {'[ê³µì§€] ' if is_notice else ''}{title[:40]}")
                    count += 1
                    if not is_notice:
                        has_general_post = True
                except Exception as e:
                    print(f"    âŒ ìƒì„¸ í˜ì´ì§€ ì‹¤íŒ¨: {title[:30]} ({e})")

            if not has_general_post:
                print(f"  ğŸ”š ë” ì´ìƒ ì¼ë°˜ ê¸€ ì—†ìŒ (page {page + 1}), ì¢…ë£Œ")
                break

            page += 1
            time.sleep(random.uniform(0.5, 1.2))
        print(f"âœ… [{dept}] ì´ ìˆ˜ì§‘: {count}ê±´")







def crawl_mainpage():
    print("\nğŸ“‚ [ë©”ì¸í˜ì´ì§€] ì‹œì‘")
    BASE_URL = "https://www.kangwon.ac.kr"
    PATH_PREFIX = "/www"
    categories = [
        {"name": "ê³µì§€ì‚¬í•­", "bbsNo": "81", "key": "277", "last_page": 5},
        {"name": "í–‰ì‚¬ì•ˆë‚´", "bbsNo": "38", "key": "279", "last_page": 2},
        {"name": "ê³µëª¨ëª¨ì§‘", "bbsNo": "345", "key": "1959", "last_page": 2},
        {"name": "ì¥í•™ê²Œì‹œíŒ", "bbsNo": "34", "key": "232", "last_page": 2},
    ]
    for cat in categories:
        for page in range(1, cat['last_page'] + 1):
            list_url = f"{BASE_URL}{PATH_PREFIX}/selectBbsNttList.do?bbsNo={cat['bbsNo']}&pageUnit=10&key={cat['key']}&pageIndex={page}"
            res = session.get(list_url, headers=HEADERS)
            time.sleep(random.uniform(0.5, 1.2))
            soup = BeautifulSoup(res.text, 'html.parser')
            rows = soup.select("tbody tr")
            for row in rows:
                if row.select_one(".notice"):
                    continue
                a_tag = row.select_one("td.subject a")
                if not a_tag:
                    continue
                title = a_tag.text.strip()
                href = a_tag.get("href", "")
                if "fnSelectBbsNttView" in href:
                    match = re.search(r"fnSelectBbsNttView\('(\d+)',\s*'(\d+)',\s*'(\d+)'\)", href)
                    if not match:
                        continue
                    bbs_no, ntt_no, key_param = match.groups()
                    detail_url = f"{BASE_URL}{PATH_PREFIX}/selectBbsNttView.do?bbsNo={bbs_no}&nttNo={ntt_no}&key={key_param}"
                else:
                    detail_url = urljoin(f"{BASE_URL}{PATH_PREFIX}/", href)
                try:
                    r = session.get(detail_url, headers=HEADERS)
                    time.sleep(random.uniform(0.5, 1.2))
                    s = BeautifulSoup(r.text, 'html.parser')
                    content_div = s.select_one("div#bbs_ntt_cn_con") or s.select_one("td.bbs_content") or s.select_one("div.bbs_content")
                    content = content_div.get_text("\n", strip=True) if content_div else "(ë³¸ë¬¸ ì—†ìŒ)"
                    date = extract_written_date(s)
                    img_tags = content_div.select("img") if content_div else []
                    photo_div = s.select_one("div.photo_area")
                    if photo_div:
                        img_tags += photo_div.select("img")
                    images = [urljoin(detail_url, img.get("src")) for img in img_tags if img.get("src")]
                    img_files = [save_image(link, os.path.join(SAVE_FOLDER, "main"), cat['bbsNo'], i) for i, link in enumerate(images)]
                    img_files = list(filter(None, img_files))
                    add_notice_if_not_duplicate(title, date, content, detail_url, ";".join(img_files))
                except Exception as e:
                    print(f"âŒ ë©”ì¸í˜ì´ì§€ ìƒì„¸ ì‹¤íŒ¨: {title[:30]} ({e})")


def crawl_library():
    print("\nğŸ“‚ [ë„ì„œê´€] ì‹œì‘")
    base_url = "https://library.kangwon.ac.kr"
    list_api = f"{base_url}/pyxis-api/1/bulletin-boards/24/bulletins"
    detail_api = f"{base_url}/pyxis-api/1/bulletins/24/{{id}}"
    per_page = 10
    seen_ids = set()
    for page in range(0, 5):
        offset = page * per_page
        params = {"offset": offset, "max": 10, "bulletinCategoryId": 1}
        try:
            res = session.get(list_api, headers=HEADERS, params=params)
            time.sleep(random.uniform(0.5, 1.2))
            data = res.json().get("data", {})
            for item in data.get("list", []):
                id_ = item['id']
                if id_ in seen_ids:
                    continue
                seen_ids.add(id_)
                title = item['title']
                detail_url = f"{base_url}/community/bulletin/notice/{id_}"
                try:
                    detail = session.get(detail_api.format(id=id_), headers=HEADERS).json().get("data", {})
                    time.sleep(random.uniform(0.5, 1.2))
                    raw_date = detail.get("dateCreated", "ì‘ì„±ì¼ ì—†ìŒ")[:10]
                    date = raw_date.replace("-", ".")
                    html = detail.get("content", "")
                    soup = BeautifulSoup(html, "html.parser")
                    content = soup.get_text("\n", strip=True)
                    images = [urljoin(base_url, img['src']) for img in soup.find_all("img") if "/pyxis-api/attachments/" in img.get('src', '')]
                    img_files = [save_image(link, os.path.join(SAVE_FOLDER, "library"), id_, i) for i, link in enumerate(images)]
                    add_notice_if_not_duplicate(title, date, content, detail_url, ";".join(filter(None, img_files)))
                except Exception as e:
                    print(f"âŒ ë„ì„œê´€ ìƒì„¸ ì‹¤íŒ¨: {title[:30]} ({e})")
        except Exception as e:
            print(f"âŒ ë„ì„œê´€ ëª©ë¡ ì‹¤íŒ¨ (offset={offset}): {e}")


def crawl_administration():
    print("\nğŸ“‚ [í–‰ì •í•™ê³¼] ì‹œì‘")
    base_url = "https://padm.kangwon.ac.kr"
    for offset in range(0, 200, 10):  # í•„ìš”ì‹œ ë²”ìœ„ í™•ì¥
        url = f"{base_url}/padm/life/notice-department.do?article.offset={offset}"
        try:
            res = session.get(url, headers=HEADERS)
            time.sleep(random.uniform(0.5, 1.2))
            soup = BeautifulSoup(res.text, 'html.parser')
        except Exception as e:
            print(f"âŒ ëª©ë¡ ìš”ì²­ ì‹¤íŒ¨ (offset={offset}): {e}")
            continue

        for idx, row in enumerate(soup.select("td.b-td-left.b-td-title")):
            a_tag = row.select_one("a")
            if not a_tag:
                continue
            title = a_tag.text.strip()
            if "ê³µì§€" in title:
                continue  # ê³µì§€ ì œì™¸
            relative = a_tag.get("href", "")
            detail_link = f"{base_url}/padm/life/notice-department.do{relative[relative.find('?'):]}" if '?' in relative else f"{base_url}/padm/life/notice-department.do"

            try:
                r = session.get(detail_link, headers=HEADERS)
                time.sleep(random.uniform(0.5, 1.2))
                s = BeautifulSoup(r.text, 'html.parser')
                content_div = s.select_one("div.b-content-box div.fr-view") or s.select_one("div.b-content-box")
                content = clean_html_keep_table(str(content_div)) if content_div else "(ë³¸ë¬¸ ì—†ìŒ)"
                date_tag = s.select_one("li.b-date-box span:nth-of-type(2)")
                date = date_tag.text.strip() if date_tag else "(ì‘ì„±ì¼ ì—†ìŒ)"

                img_links = []
                for a in s.select("div.b-file-box a.file-down-btn"):
                    name = a.text.strip()
                    file_href = a.get("href", "")
                    if not file_href:
                        continue
                    full_link = base_url + "/padm/life/notice-department.do" + file_href if file_href.startswith('?') else file_href
                    if name.lower().endswith(('.png', '.jpg', '.jpeg')):
                        img_links.append(full_link)

                img_files = [save_image(link, os.path.join(SAVE_FOLDER, "admin"), offset + idx, i) for i, link in enumerate(img_links)]
                img_files = list(filter(None, img_files))
                add_notice_if_not_duplicate(title, date, content, detail_link, ";".join(img_files))

            except Exception as e:
                print(f"âŒ í–‰ì •í•™ê³¼ ìƒì„¸ ì‹¤íŒ¨: {title[:30]} ({e})")


def crawl_engineering():
    print("\nğŸ“‚ [ê³µí•™êµìœ¡í˜ì‹ ì„¼í„°] ì‹œì‘")
    base_url = "https://icee.kangwon.ac.kr"
    for page in range(1, 10):  # í•„ìš”ì‹œ ë²”ìœ„ í™•ì¥
        url = f"{base_url}/index.php?mt=page&mp=5_1&mm=oxbbs&oxid=1&cpage={page}"
        try:
            res = session.get(url, headers=HEADERS)
            time.sleep(random.uniform(0.5, 1.2))
            soup = BeautifulSoup(res.text, 'html.parser')
        except Exception as e:
            print(f"âŒ ëª©ë¡ ìš”ì²­ ì‹¤íŒ¨ (page={page}): {e}")
            continue

        for row in soup.select("table.bbs_list tbody tr"):
            a_tag = row.select_one("td.tit a")
            if not a_tag:
                continue
            title = a_tag.text.strip()
            href = urljoin(base_url, a_tag['href'])
            raw_date = row.select_one("td.dt").text.strip().replace("-", ".")

            try:
                r = session.get(href, headers=HEADERS)
                time.sleep(random.uniform(0.5, 1.2))
                s = BeautifulSoup(r.text, 'html.parser')
                content_div = s.select_one("div.view_cont") or s.select_one("div.note")
                content = content_div.get_text("\n", strip=True) if content_div else "(ë³¸ë¬¸ ì—†ìŒ)"

                imgs = content_div.find_all("img") if content_div else []
                img_files = []
                for i, img in enumerate(imgs):
                    src = img.get("src")
                    if src and not src.startswith("data:image"):
                        img_files.append(save_image(
                            urljoin(href, src),
                            os.path.join(SAVE_FOLDER, "engineering"),
                            page, i
                        ))
                img_files = list(filter(None, img_files))
                add_notice_if_not_duplicate(title, raw_date, content, href, ";".join(img_files))

            except Exception as e:
                print(f"âŒ ê³µí•™êµìœ¡í˜ì‹ ì„¼í„° ìƒì„¸ ì‹¤íŒ¨: {title[:30]} ({e})")


def crawl_international():
    print("\nğŸ“‚ [êµ­ì œêµë¥˜ì²˜] ì‹œì‘")
    base_url = "https://oiaknu.kangwon.ac.kr"
    for offset in range(0, 30, 10):
        url = f"{base_url}/oiaknu/notice.do?article.offset={offset}"
        try:
            res = session.get(url, headers=HEADERS)
            time.sleep(random.uniform(0.5, 1.2))
            soup = BeautifulSoup(res.text, 'html.parser')
        except Exception as e:
            print(f"âŒ ëª©ë¡ ìš”ì²­ ì‹¤íŒ¨ (offset={offset}): {e}")
            continue

        rows = soup.select("td.b-td-left.b-td-title")
        if not rows:
            break

        for idx, row in enumerate(rows):
            a_tag = row.select_one("a")
            if not a_tag:
                continue
            title = a_tag.get("title", "").replace("ìì„¸íˆ ë³´ê¸°", "").strip()
            relative = a_tag.get("href", "")
            detail_link = f"{base_url}/oiaknu/notice.do{relative[relative.find('?'):]}" if '?' in relative else f"{base_url}/oiaknu/notice.do"

            try:
                r = session.get(detail_link, headers=HEADERS)
                time.sleep(random.uniform(0.5, 1.2))
                s = BeautifulSoup(r.text, 'html.parser')
                content_div = s.select_one("div.b-content-box div.fr-view") or s.select_one("div.b-content-box")
                content = clean_html_keep_table(str(content_div)) if content_div else "(ë³¸ë¬¸ ì—†ìŒ)"
                date_tag = s.select_one("li.b-date-box span:nth-of-type(2)")
                date = date_tag.text.strip() if date_tag else "(ì‘ì„±ì¼ ì—†ìŒ)"

                img_links = []
                for a in s.select("div.b-file-box a.file-down-btn"):
                    name = a.text.strip()
                    file_href = a.get("href", "")
                    if not file_href:
                        continue
                    full_link = (
                        base_url + "/oiaknu/notice.do" + file_href
                        if file_href.startswith('?') else file_href
                    )
                    if name.lower().endswith(('.png', '.jpg', '.jpeg')):
                        img_links.append(full_link)

                img_files = [
                    save_image(link, os.path.join(SAVE_FOLDER, "international"), offset + idx, i)
                    for i, link in enumerate(img_links)
                ]
                img_files = list(filter(None, img_files))
                add_notice_if_not_duplicate(title, date, content, detail_link, ";".join(img_files))

            except Exception as e:
                print(f"âŒ êµ­ì œêµë¥˜ì²˜ ìƒì„¸ ì‹¤íŒ¨: {title[:30]} ({e})")


college_intro_pages = [
    {'college_name': 'ê°„í˜¸ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1782&'},
    {'college_name': 'ê²½ì˜ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1752&'},
    {'college_name': 'ë†ì—…ìƒëª…ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1758&'},
    {'college_name': 'ë™ë¬¼ìƒëª…ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1761&'},
    {'college_name': 'ë¬¸í™”ì˜ˆìˆ  ê³µê³¼ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1912&'},
    {'college_name': 'ì‚¬ë²”ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1767&'},
    {'college_name': 'ì‚¬íšŒê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1770&'},
    {'college_name': 'ì‚°ë¦¼í™˜ê²½ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1773&'},
    {'college_name': 'ìˆ˜ì˜ê³¼ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1776&'},
    {'college_name': 'ì•½í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1779&'},
    {'college_name': 'ì˜ê³¼ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1975&'},
    {'college_name': 'ì˜ìƒëª…ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1785&'},
    {'college_name': 'ì¸ë¬¸ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1788&'},
    {'college_name': 'ìì—°ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1791&'},
    {'college_name': 'ITëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1794&'},
]


# ì¶”ê°€ ë§¤í•‘: í•™ê³¼ ì´ë¦„ -> ê³µì§€ì‚¬í•­ URL
manual_board_mapping = {
    "AIìœµí•©í•™ê³¼": "https://ai.kangwon.ac.kr/ai/community/notice.do",
    "ë””ì§€í„¸ë°€ë¦¬í„°ë¦¬í•™ê³¼": "https://military.kangwon.ac.kr/military/professor/notice.do",
    "ììœ ì „ê³µí•™ë¶€": "https://liberal.kangwon.ac.kr/liberal/community/notice.do",
    "ê¸€ë¡œë²Œìœµí•©í•™ë¶€": "https://globalconvergence.kangwon.ac.kr/globalconvergence/info/undergraduate-community.do",
    "ë¯¸ë˜ìœµí•©ê°€ìƒí•™ê³¼": "https://multimajor.kangwon.ac.kr/multimajor/community/notice.do",
    "ë™ë¬¼ì‚°ì—…ìœµí•©í•™ê³¼": "https://animal.kangwon.ac.kr/animal/community/notice.do"
    # í•„ìš”ì‹œ ì—¬ê¸°ì— ì¶”ê°€
}


if __name__ == "__main__":
    print("\nğŸš€ ê°•ì›ëŒ€ ì „ì²´ ê³µì§€ í¬ë¡¤ë§ ì‹œì‘")
    # crawl_mainpage()
    # crawl_library()
    # crawl_engineering()
    # crawl_administration()
    # crawl_international()

    boards = extract_notice_board_urls(college_intro_pages)
    if boards:
        crawl_all_departments(boards)



    with open(CSV_FILE, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=["ì œëª©", "ì‘ì„±ì¼", "ë³¸ë¬¸ë‚´ìš©", "ë§í¬", "ì‚¬ì§„"])
        writer.writeheader()
        writer.writerows(all_data)
    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {CSV_FILE}")