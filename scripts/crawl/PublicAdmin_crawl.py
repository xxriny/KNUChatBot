from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import re
import csv

# ===== í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì • =====
options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')

driver = webdriver.Chrome(options=options)

# ===== URL ì„¤ì • =====
base_url = "https://padm.kangwon.ac.kr"
list_url = f"{base_url}/padm/life/notice-department.do"

# ===== HTML íƒœê·¸ ì œê±° ë° í‘œ ì²˜ë¦¬ =====
def clean_html_keep_table(raw_html):
    soup = BeautifulSoup(raw_html, 'html.parser')
    output_text = ''
    tables = soup.find_all('table')
    for table in tables:
        table_text = extract_table_text(table)
        if table_text.strip():
            output_text += table_text + '\n'
        table.decompose()
    for elem in soup.find_all(['p', 'div']):
        text = elem.get_text(strip=True)
        if text:
            output_text += text + '\n'
    return output_text.strip()

def extract_table_text(table):
    rows = table.find_all('tr')
    table_text = ''
    for row in rows:
        cols = row.find_all(['td', 'th'])
        valid_cols = [col.get_text(strip=True) for col in cols if col.get_text(strip=True)]
        if valid_cols:
            row_text = ' | '.join(valid_cols)
            table_text += row_text + '\n'
    return table_text

# ===== ê³µì§€ ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§ =====
def crawl_notice_list(offset=0):
    driver.get(f"{list_url}?article.offset={offset}")
    time.sleep(2)

    notices = []
    rows = driver.find_elements(By.CSS_SELECTOR, 'td.b-td-left.b-td-title')

    for row in rows:
        try:
            title_box = row.find_element(By.CSS_SELECTOR, 'div.b-title-box')

            if 'b-notice' in title_box.get_attribute('class'):
                continue

            link_tag = title_box.find_element(By.CSS_SELECTOR, 'a')
            title = link_tag.text.strip()
            href = link_tag.get_attribute('href')
            detail_url = base_url + "/padm/life/notice-department.do" + href[href.find('?'):]

            notices.append({'title': title, 'url': detail_url})
        except Exception as e:
            print("[!] ë¦¬ìŠ¤íŠ¸ í•­ëª© íŒŒì‹± ì‹¤íŒ¨:", e)
            continue

    return notices

# ===== ê³µì§€ ë³¸ë¬¸ í¬ë¡¤ë§ =====
def crawl_notice_detail(url):
    driver.get(url)

    try:
        date_element = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, 'div.b-etc-box li.b-date-box span:nth-child(2)'))
        )
        date_text = date_element.text.strip()
    except:
        date_text = "(ì‘ì„±ì¼ ì—†ìŒ)"

    selector_candidates = [
        'div.b-content-box div.fr-view',
        'div.b-content-box'
    ]

    content_text = ""
    for selector in selector_candidates:
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, selector))
            )
            element = driver.find_element(By.CSS_SELECTOR, selector)
            content_html = element.get_attribute('innerHTML')
            content_text = clean_html_keep_table(content_html)
            if content_text.strip():
                break
        except:
            continue

    if not content_text.strip():
        content_text = "(ë³¸ë¬¸ ì—†ìŒ)"

    img_links = []
    try:
        file_elements = driver.find_elements(By.CSS_SELECTOR, 'div.b-file-box a.file-down-btn')
        for file in file_elements:
            file_href = file.get_attribute('href')
            file_name = file.text.strip()
            if file_href and file_name:
                full_link = base_url + file_href if file_href.startswith('?') else file_href
                if any(file_name.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg']):
                    img_links.append(full_link)
    except:
        pass

    return date_text, content_text, img_links

# ===== ë©”ì¸ ì‹¤í–‰ =====
if __name__ == "__main__":
    all_notices = []
    total_articles = 7218 
    articles_per_page = 10

    for offset in range(0, total_articles, articles_per_page):
        print(f"\nğŸ“„ í˜„ì¬ í˜ì´ì§€ offset: {offset} (ê³µì§€ {offset+1} ~ {offset+10})")
        notices = crawl_notice_list(offset=offset)

        for idx, notice in enumerate(notices, start=1):
            title = notice['title']
            url = notice['url']
            date, content, img_links = crawl_notice_detail(url)

            all_notices.append({
                'ì œëª©': title,
                'ì‘ì„±ì¼': date,
                'ë³¸ë¬¸': content,
                'url': url  ,
                'ì´ë¯¸ì§€íŒŒì¼ ë§í¬': ', '.join(img_links)
            })

            print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {title} ({offset+idx}/{total_articles})")
            time.sleep(2)  # âœ… ì„œë²„ ë¶€í•˜ ë°©ì§€

    driver.quit()

    # âœ… CSV íŒŒì¼ë¡œ ì €ì¥
    keys = ['ì œëª©', 'ì‘ì„±ì¼', 'ë³¸ë¬¸', 'url', 'ì´ë¯¸ì§€íŒŒì¼ ë§í¬']
    with open('kangwon_notices_total.csv', 'w', newline='', encoding='utf-8-sig') as f:
        dict_writer = csv.DictWriter(f, fieldnames=keys)
        dict_writer.writeheader()
        dict_writer.writerows(all_notices)

    print(f"\nâœ… ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(all_notices)}ê°œ ì €ì¥ ì™„ë£Œ.")