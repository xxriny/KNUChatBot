import os
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import time
import requests
from urllib.parse import urljoin
import re
from tqdm import tqdm # ì§„í–‰ë¥  í‘œì‹œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë‹¤ì‹œ í™œì„±í™”
import datetime # ë‚ ì§œ ì‚¬ìš© (í˜„ì¬ëŠ” ìµœì í™”ì— ì§ì ‘ ì‚¬ìš© ì•ˆí•¨)

from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException, WebDriverException

# --- ì„¤ì • ---
CHROMEDRIVER_PATH = r"D:\chromedriver-win64\chromedriver.exe"
CSV_FILENAME = "kangwon_wwwk_notices_cumulative_final.csv" # ìµœì¢… ëˆ„ì  ë°ì´í„° CSV íŒŒì¼ëª…
IMAGE_FOLDER = "images_content" # ì´ë¯¸ì§€ ì €ì¥ í´ë”
PROCESSED_URLS_FILE = "processed_urls.txt" # ì²˜ë¦¬ëœ URL ê¸°ë¡ íŒŒì¼

TARGETS = [
    {'site': 'wwwk', 'name': 'ê³µì§€ì‚¬í•­', 'bbsNo': '81', 'key': '277', 'searchCtgry': '%EC%A0%84%EC%B2%B4%40%40%EC%B6%98%EC%B2%9C'},
    {'site': 'wwwk', 'name': 'í–‰ì‚¬ì•ˆë‚´', 'bbsNo': '38', 'key': '279', 'searchCtgry': '%EC%A0%84%EC%B2%B4%40%40%EC%B6%98%EC%B2%9C'},
    {'site': 'wwwk', 'name': 'ê³µëª¨ëª¨ì§‘', 'bbsNo': '345', 'key': '1959', 'searchCtgry': ''},
    {'site': 'wwwk', 'name': 'ì¥í•™ê²Œì‹œíŒ', 'bbsNo': '34', 'key': '232', 'searchCtgry': ''},
]

# --- ì´ë¯¸ì§€ ì €ì¥ í´ë” í™•ì¸ ë° ì ˆëŒ€ ê²½ë¡œ ---
IMAGE_FOLDER_ABSPATH = ""
# ... (í´ë” ìƒì„± ë° ê¶Œí•œ í™•ì¸ ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼) ...
if not os.path.exists(IMAGE_FOLDER):
    try: os.makedirs(IMAGE_FOLDER)
    except OSError as e: print(f"âŒ Error creating directory '{IMAGE_FOLDER}': {e}"); exit()
try:
    IMAGE_FOLDER_ABSPATH = os.path.abspath(IMAGE_FOLDER)
    print(f"â„¹ï¸ Images will be saved to: {IMAGE_FOLDER_ABSPATH}")
    test_file_path = os.path.join(IMAGE_FOLDER_ABSPATH, "write_test.tmp")
    with open(test_file_path, "w") as f_test: f_test.write("test"); os.remove(test_file_path)
    # print(f"  âœ… Write permission seems OK.")
except Exception as e_perm: print(f"  âŒ WARNING: Write permission check failed: {e_perm}")


# --- ì›¹ë“œë¼ì´ë²„ ì„¤ì • ---
options = Options()
# ... (ì˜µì…˜ ì„¤ì •ì€ ì´ì „ê³¼ ë™ì¼) ...
options.add_argument("--disable-gpu"); options.add_argument("--no-sandbox"); options.add_argument("--start-maximized")
# options.add_argument("--headless") # ì „ì²´ í¬ë¡¤ë§ ì‹œ ë¹„ì¶”ì²œ
options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
service = Service(CHROMEDRIVER_PATH)

# === ì²˜ë¦¬ëœ URL ë¶ˆëŸ¬ì˜¤ê¸° ===
processed_urls = set()
# ... (URL ë¶ˆëŸ¬ì˜¤ê¸° ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼) ...
try:
    if os.path.exists(PROCESSED_URLS_FILE):
        with open(PROCESSED_URLS_FILE, 'r', encoding='utf-8') as f_urls:
            processed_urls = set(line.strip() for line in f_urls if line.strip())
        print(f"âœ… Loaded {len(processed_urls)} previously processed URLs from '{PROCESSED_URLS_FILE}'.")
    else: print(f"â„¹ï¸ '{PROCESSED_URLS_FILE}' not found. Starting fresh.")
except Exception as e_load: print(f"âŒ Error loading processed URLs: {e_load}")

# --- ì „ì²´ ì§„í–‰ ìƒí™© ì¹´ìš´í„° ---
total_processed_overall = 0 # ì´ë²ˆ ì‹¤í–‰ ìƒˆë¡œ ì²˜ë¦¬ëœ ìˆ˜
total_attempted = 0       # ì´ë²ˆ ì‹¤í–‰ ì²˜ë¦¬ ì‹œë„ ìˆ˜
total_skipped_duplicates = 0 # ì´ë²ˆ ì‹¤í–‰ ì¤‘ë³µ ê±´ë„ˆë›´ ìˆ˜

# --- ê²Œì‹œíŒ í¬ë¡¤ë§ í•¨ìˆ˜ ---
def scrape_wwwk_category(driver, category_info):
    # global total_processed_overall, total_attempted, total_skipped_duplicates
    category_results = []
    category_name = category_info['name']; bbsNo = category_info['bbsNo']; key = category_info['key']
    category_searchCtgry = category_info.get('searchCtgry', ''); wwwk_base_url = "https://wwwk.kangwon.ac.kr"
    page_index = 1; processed_count_in_category_this_run = 0; skipped_duplicates_this_run = 0; skipped_notices_after_p1 = 0
    newly_added_in_category = 0

    while True: # í˜ì´ì§€ ìˆœíšŒ
        # === í˜ì´ì§€ ì œí•œ ë¡œì§ ì œê±° ===
        # if page_index > 3: break

        print(f"\n{'='*15} {category_name} - í˜ì´ì§€ {page_index} í¬ë¡¤ë§ ì‹œë„ {'='*15}")
        search_param = f"&searchCtgry={category_searchCtgry}" if category_searchCtgry else ""
        list_url = f"{wwwk_base_url}/www/selectBbsNttList.do?bbsNo={bbsNo}&pageUnit=10{search_param}&key={key}&pageIndex={page_index}"

        try: # ëª©ë¡ í˜ì´ì§€ ë¡œë”©
            driver.get(list_url)
            WebDriverWait(driver, 30).until( EC.presence_of_element_located((By.CSS_SELECTOR, "tbody tr")))
        except TimeoutException: print(f"âŒ í˜ì´ì§€ {page_index} ë¡œë”© ì‹œê°„ ì´ˆê³¼: {list_url}"); break

        notices_on_page = [] # í˜„ì¬ í˜ì´ì§€ì—ì„œ ìƒì„¸ ì²˜ë¦¬í•  í›„ë³´
        rows = []
        new_url_found_on_this_page = False # ìµœì í™”ìš© í”Œë˜ê·¸

        try: # ëª©ë¡ ì²˜ë¦¬
            rows = driver.find_elements(By.CSS_SELECTOR, "tbody tr") # ëª¨ë“  í–‰ ê°€ì ¸ì˜¤ê¸°
            if not rows: print(f"  âœ… í˜ì´ì§€ {page_index}ì— ê²Œì‹œê¸€ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. '{category_name}' í¬ë¡¤ë§ ì¢…ë£Œ."); break

            # print(f"  - í˜ì´ì§€ {page_index} ì—ì„œ {len(rows)}ê°œ í–‰ ë°œê²¬...")

            for row_idx, row in enumerate(rows):
                try:
                    cells = row.find_elements(By.TAG_NAME, "td")
                    if len(cells) < 3: continue

                    first_cell_content = "N/A"; is_notice = False
                    try:
                        first_cell_content = cells[0].text.strip()
                        is_notice = not first_cell_content.isdigit()
                    except IndexError: continue

                    # ê³µì§€ ê±´ë„ˆë›°ê¸° (ì²« í˜ì´ì§€ ì œì™¸)
                    if is_notice and page_index != 1:
                        skipped_notices_after_p1 += 1; continue

                    # ì •ë³´ ì¶”ì¶œ (ì¼ë°˜ê¸€ ë˜ëŠ” ì²« í˜ì´ì§€ ê³µì§€)
                    title = "ì œëª© ì—†ìŒ"; detail_url = None; href = None; date_from_list = "ë‚ ì§œ ì°¾ê¸° ì‹¤íŒ¨"
                    try:
                        title_element = cells[2].find_element(By.CSS_SELECTOR, "a")
                        title = title_element.text.strip(); href = title_element.get_attribute('href')
                    except Exception: continue
                    for cell in cells: # ë‚ ì§œ ì°¾ê¸°
                        cell_text = cell.text.strip()
                        if re.match(r'^\d{4}[.-]\d{2}[.-]\d{2}$', cell_text): date_from_list = cell_text.replace('-', '.'); break
                    # URL ì²˜ë¦¬
                    if href and title:
                        if href.startswith('javascript:fnSelectBbsNttView'):
                            try: ntt_no = href.split("'")[1]
                            except IndexError: detail_url = None
                            else: detail_url = f"{wwwk_base_url}/www/selectBbsNttView.do?bbsNo={bbsNo}&nttNo={ntt_no}&key={key}"
                        elif href.startswith('/'): detail_url = urljoin(wwwk_base_url, href)
                        elif href.startswith('http'): detail_url = href
                        else: detail_url = None
                    if detail_url:
                        notices_on_page.append({'title': title, 'url': detail_url, 'date': date_from_list})
                except Exception as e_row: print(f"    âŒ í–‰ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ (Row {row_idx+1}): {e_row}")

        except Exception as e: print(f"  âŒ ëª©ë¡ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (í˜ì´ì§€ {page_index}): {e}"); break

        if not notices_on_page: # ì²˜ë¦¬í•  í›„ë³´ê°€ ì—†ìœ¼ë©´
             if rows: print(f"  âš ï¸ í˜ì´ì§€ {page_index}ì—ì„œ ì²˜ë¦¬í•  ê²Œì‹œê¸€ í›„ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
             else: print(f"  âœ… í˜ì´ì§€ {page_index} ì—ì„œ ì²˜ë¦¬í•  ìœ íš¨í•œ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤. '{category_name}' í¬ë¡¤ë§ ì¢…ë£Œ."); break
             # === ìµœì í™”: í˜„ì¬ í˜ì´ì§€ì— í›„ë³´ê°€ ì—†ì—ˆê³ , 1í˜ì´ì§€ê°€ ì•„ë‹ˆë©´ ì¢…ë£Œ ===
             if page_index > 1:
                  print(f"  ---> No processable items found on page {page_index}. Stopping crawl for '{category_name}'.")
                  break
             page_index += 1; time.sleep(1); continue # ì²« í˜ì´ì§€ëŠ” ë¹„ì–´ë„ ë‹¤ìŒ í˜ì´ì§€ ì‹œë„

        # === ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ (URL ì¤‘ë³µ ì œê±° í¬í•¨) ===
        print(f"  - í˜ì´ì§€ {page_index}: {len(notices_on_page)}ê°œ ê²Œì‹œê¸€ í›„ë³´ ìƒì„¸ ì²˜ë¦¬ ì‹œì‘...")
        processed_on_this_page = 0
        for notice_info in tqdm(notices_on_page, desc=f"  Processing Page {page_index} Items", leave=False, ncols=100): # tqdm ë‹¤ì‹œ ì‚¬ìš©
            global total_processed_overall, total_attempted, total_skipped_duplicates
            total_attempted += 1
            detail_url = notice_info['url']; title = notice_info['title']

            if detail_url in processed_urls:
                skipped_duplicates_this_run += 1; total_skipped_duplicates += 1; continue

            processed_urls.add(detail_url)
            processed_count_in_category_this_run += 1
            total_processed_overall += 1
            new_url_found_on_this_page = True # â˜…â˜…â˜… ìƒˆ URL ì°¾ìŒ í”Œë˜ê·¸ ì„¤ì • â˜…â˜…â˜…
            date = notice_info['date']

            body = "ë³¸ë¬¸ ë‚´ìš© ì—†ìŒ"; local_image_filenames = []
            try:
                driver.get(detail_url)
                WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                # ë³¸ë¬¸/ì´ë¯¸ì§€ ì²˜ë¦¬ (ì´ì „ê³¼ ë™ì¼)
                try: # Body
                    soup = BeautifulSoup(driver.page_source, 'html.parser'); content_div_bs = soup.select_one("div#bbs_ntt_cn_con"); body_parts = [];
                    if content_div_bs: body = content_div_bs.get_text(separator='\n', strip=True);
                    if not body: body = "ë³¸ë¬¸ ë‚´ìš© ì—†ìŒ"
                except Exception: body = "ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜"
                try: # Image
                    images_selenium = driver.find_elements(By.TAG_NAME, "img")
                    for idx, img_selenium in enumerate(images_selenium):
                        img_url_raw = None; img_url = None; final_filename = None
                        try:
                            img_url_raw = img_selenium.get_attribute('src')
                            if not img_url_raw or img_url_raw.startswith('data:image') or '/DATA/bbs/' not in img_url_raw: continue
                            img_url = urljoin(detail_url, img_url_raw);
                            if not img_url.startswith('http'): continue
                            img_data = None
                            try: # Download
                                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
                                img_response = requests.get(img_url, timeout=20, headers=headers); img_response.raise_for_status(); img_data = img_response.content
                            except Exception: continue
                            try: # Save
                                img_filename_base = os.path.basename(img_url.split('?')[0]); valid_chars = "-_.() abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"; img_filename_base = ''.join(c for c in img_filename_base if c in valid_chars).strip()
                                if not img_filename_base or len(img_filename_base) > 100: img_filename_base = f"image_{total_processed_overall}_{idx}"
                                _, ext = os.path.splitext(img_url.split('?')[0]);
                                if not ext: ext = '.jpg'
                                final_filename = f"{os.path.splitext(img_filename_base)[0]}_{int(time.time() * 1000)}{ext}"; img_save_path = os.path.join(IMAGE_FOLDER_ABSPATH, final_filename)
                                with open(img_save_path, 'wb') as f: f.write(img_data)
                                if final_filename not in local_image_filenames: local_image_filenames.append(final_filename)
                            except Exception as e_save: print(f"       âŒ File save FAILED for '{final_filename}' (from {img_url}): {e_save}")
                        except Exception: pass
                except Exception: pass

                category_results.append([title, body, date, detail_url, ", ".join(local_image_filenames)])
                processed_on_this_page += 1

            except Exception as e_detail:
                 print(f"     âŒ Error processing detail page for {title[:20]}...: {e_detail}")
            finally: time.sleep(0.3)

            if total_processed_overall % 50 == 0 and total_processed_overall > 0: current_time = time.strftime("%H:%M:%S"); print(f"\nâœ¨ [{current_time}] --- ì´ {total_processed_overall}ê°œ ì‹ ê·œ ê²Œì‹œê¸€ ì²˜ë¦¬ ì™„ë£Œ (ëˆ„ì ) --- âœ¨\n")

        print(f"  ğŸ“Š í˜ì´ì§€ {page_index} ì™„ë£Œ. ì‹ ê·œ ì²˜ë¦¬: {processed_on_this_page}ê°œ. (P2+ ê³µì§€ {skipped_notices_after_p1}ê°œ, URLì¤‘ë³µ {skipped_duplicates_this_run}ê°œ ê±´ë„ˆ<0xEB><0x8B>)")

        # === ìµœì í™”: í˜„ì¬ í˜ì´ì§€ì—ì„œ ìƒˆ URLì´ ì—†ì—ˆê³ , 1í˜ì´ì§€ê°€ ì•„ë‹ˆë©´ ì¢…ë£Œ ===
        if not new_url_found_on_this_page and page_index > 1:
             print(f"  ---> No new posts found on page {page_index}. Stopping crawl for '{category_name}'.")
             break

        page_index += 1; time.sleep(1) # ë‹¤ìŒ í˜ì´ì§€ë¡œ

    print(f"\n### {category_name} í¬ë¡¤ë§ ì™„ë£Œ (í˜ì´ì§€ {page_index-1}ê¹Œì§€ í™•ì¸) ###")
    return category_results, skipped_duplicates_this_run


# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
newly_added_results = [] # ì´ë²ˆ ì‹¤í–‰ì—ì„œ ìƒˆë¡œ ì¶”ê°€ëœ ê²°ê³¼ë§Œ ì €ì¥
total_skipped_this_run = 0
driver = None; start_time = time.time()
try:
    try: from tqdm import tqdm
    except ImportError: print("Tip: 'pip install tqdm' to see progress bars."); tqdm = lambda x, **kwargs: x
    driver = webdriver.Chrome(service=service, options=options)
    print("âœ… WebDriver ì‹œì‘ë¨.")
    for target_info in TARGETS:
        category_name = target_info['name']; print(f"\n{'='*20} ì¹´í…Œê³ ë¦¬ ì‹œì‘: {category_name} {'='*20}")
        site_type = target_info.get('site'); category_results = []; skipped_count = 0
        if site_type == 'wwwk':
            category_results, skipped_count = scrape_wwwk_category(driver, target_info) # ë°˜í™˜ê°’ 2ê°œ ë°›ìŒ
        else: print(f"â— ì•Œ ìˆ˜ ì—†ëŠ” ì‚¬ì´íŠ¸ íƒ€ì…: {site_type}")
        if category_results:
            newly_added_results.extend(category_results) # ìƒˆë¡œ ì°¾ì€ ê²°ê³¼ë§Œ ëˆ„ì 
            print(f"\nâœ… '{category_name}' ì¹´í…Œê³ ë¦¬ì—ì„œ {len(category_results)}ê°œì˜ **ìƒˆë¡œìš´** ê²Œì‹œê¸€ ë°œê²¬. (ì¤‘ë³µ URL ê±´ë„ˆ<0xEB><0x8B> {skipped_count}ê°œ)")
        else: print(f"âš ï¸ '{category_name}' ì¹´í…Œê³ ë¦¬ì—ì„œ ìƒˆë¡œìš´ ê²Œì‹œê¸€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì¤‘ë³µ URL ê±´ë„ˆ<0xEB><0x8B> {skipped_count}ê°œ)")
        total_skipped_this_run += skipped_count
        print(f"\n... ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì „ ì ì‹œ ëŒ€ê¸° ({category_name} ì™„ë£Œ, 1ì´ˆ) ...\n"); time.sleep(1) # ëŒ€ê¸° ì‹œê°„ ì¤„ì„
except Exception as e_main:
    print(f"\nğŸš¨ ë©”ì¸ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e_main}")
    import traceback; traceback.print_exc()
finally:
    # === ì—…ë°ì´íŠ¸ëœ URL ëª©ë¡ íŒŒì¼ì— ì €ì¥ ===
    try:
        print(f"\nğŸ’¾ Saving {len(processed_urls)} processed URLs to '{PROCESSED_URLS_FILE}'...")
        with open(PROCESSED_URLS_FILE, 'w', encoding='utf-8') as f_urls_out:
            for url in sorted(list(processed_urls)): f_urls_out.write(url + '\n')
        print(f"âœ… Successfully saved processed URLs.")
    except Exception as e_save_urls: print(f"âŒ Error saving processed URLs: {e_save_urls}")
    if driver:
        try: driver.quit()
        except Exception as e_quit: print(f"âŒ WebDriver ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e_quit}")
        else: print("\nâœ… WebDriver ì¢…ë£Œë¨.")

# --- ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥ ---
end_time = time.time(); elapsed_time = end_time - start_time
print(f"\n{'='*20} í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ (ì „ì²´ í˜ì´ì§€) {'='*20}") # ë¡œê·¸ ìˆ˜ì •
print(f"â±ï¸ ì´ ì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f} ì´ˆ ({elapsed_time/60:.2f} ë¶„)"); print(f"ğŸ”„ ì²˜ë¦¬ ì‹œë„í•œ ì´ ê²Œì‹œê¸€ ìˆ˜: {total_attempted}");
print(f"â­ï¸ ê±´ë„ˆ<0xEB><0x9D<0x80 ì¤‘ë³µ URL ìˆ˜: {total_skipped_this_run}"); print(f"âœ¨ ì´ë²ˆ ì‹¤í–‰ ìƒˆë¡œ ì¶”ê°€ëœ ê²Œì‹œê¸€ ìˆ˜: {len(newly_added_results)}")
print(f"ğŸ’¾ ìµœì¢… ëˆ„ì ëœ ê³ ìœ  URL ìˆ˜: {len(processed_urls)}")

if newly_added_results:
    try:
        df = pd.DataFrame(newly_added_results, columns=["ì œëª©", "ë³¸ë¬¸", "ì‘ì„±ì¼", "ë§í¬", "ì‚¬ì§„_íŒŒì¼ëª…"])
        df_ordered = df[["ì œëª©", "ì‘ì„±ì¼", "ë³¸ë¬¸", "ë§í¬", "ì‚¬ì§„_íŒŒì¼ëª…"]]; df_ordered.columns = ["ì œëª©", "ì‘ì„±ì¼", "ë³¸ë¬¸ë‚´ìš©", "ë§í¬", "ì‚¬ì§„"]
        # === CSV ì €ì¥ ë°©ì‹: ì¶”ê°€ (Append) ===
        is_new_file = not os.path.exists(CSV_FILENAME) # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        df_ordered.to_csv(CSV_FILENAME, mode='a', index=False, header=is_new_file, encoding='utf-8-sig')
        print(f"\nâœ… ì´ë²ˆ ì‹¤í–‰ì—ì„œ ì°¾ì€ {len(newly_added_results)}ê°œì˜ ìƒˆë¡œìš´ ê²Œì‹œê¸€ì„ '{CSV_FILENAME}'ì— **ì¶”ê°€í–ˆìŠµë‹ˆë‹¤**.")
        if is_new_file: print(f"   (ìƒˆë¡œìš´ CSV íŒŒì¼ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.)")
        else: print(f"   (ê¸°ì¡´ CSV íŒŒì¼ì— ì´ì–´ì„œ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.)")

        try:
            image_files_count = len([name for name in os.listdir(IMAGE_FOLDER) if os.path.isfile(os.path.join(IMAGE_FOLDER, name))])
            print(f"ğŸ–¼ï¸ '{IMAGE_FOLDER}' í´ë”ì— ì•½ {image_files_count}ê°œì˜ ì´ë¯¸ì§€ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except FileNotFoundError: print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ í´ë” '{IMAGE_FOLDER}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except ImportError: print(f"\nâŒ CSV ì €ì¥ì„ ìœ„í•´ 'pandas' ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    except Exception as e_csv: print(f"\nâŒ CSV íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e_csv}")
else: print(f"\nâœ… ì´ë²ˆ ì‹¤í–‰ì—ì„œ ìƒˆë¡œ ì¶”ê°€ëœ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")