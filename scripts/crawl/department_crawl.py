# -*- coding: utf-8 -*-
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse
import os
import math
import time
import hashlib
import re
import pandas as pd
from tqdm import tqdm
import traceback

# --- 1. ê¸°ë³¸ ì„¤ì • ---
try:
    script_path = os.path.abspath(__file__)
    script_dir = os.path.dirname(script_path)
    project_root = os.path.abspath(os.path.join(script_dir, '..', '..'))
except NameError:
    print("âš ï¸ __file__ ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
    project_root = os.getcwd()

DATA_FOLDER = os.path.join(project_root, 'data')
IMAGE_FOLDER_NAME = "images_content"
IMAGE_FOLDER = os.path.join(DATA_FOLDER, IMAGE_FOLDER_NAME)
CSV_FILENAME = "kangwon_all_dept_notices_beta_all_pages.csv"
CSV_FILEPATH = os.path.join(DATA_FOLDER, CSV_FILENAME)
os.makedirs(IMAGE_FOLDER, exist_ok=True)
print(f"â„¹ï¸ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
print(f"â„¹ï¸ ë°ì´í„° í´ë”: {DATA_FOLDER}")
print(f"â„¹ï¸ ì´ë¯¸ì§€ í´ë”: {IMAGE_FOLDER}")
print(f"â„¹ï¸ CSV ì €ì¥ ê²½ë¡œ: {CSV_FILEPATH}")

college_intro_pages = [
    {'college_name': 'ê°„í˜¸ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1782&'},
    {'college_name': 'ê²½ì˜ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1752&'},
    {'college_name': 'ë†ì—…ìƒëª…ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1758&'},
    {'college_name': 'ë™ë¬¼ìƒëª…ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1761&'},
    {'college_name': 'ë¬¸í™”ì˜ˆìˆ  ê³µê³¼ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1912&'},
    {'college_name': 'ì‚¬ë²”ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1767&'},
    {'college_name': 'ì‚¬íšŒê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1770&'},
    {'college_name': 'ì‚°ë¦¼í™˜ê²½ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1773&'},
    {'college_name': 'ìˆ˜ì˜ê³¼ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1776&'},
    {'college_name': 'ì•½í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1779&'},
    {'college_name': 'ì˜ê³¼ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1975&'},
    {'college_name': 'ì˜ìƒëª…ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1785&'},
    {'college_name': 'ì¸ë¬¸ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1788&'},
    {'college_name': 'ìì—°ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1791&'},
    {'college_name': 'ITëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1794&'},
]
REQUEST_DELAY = 0.3
HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
DEFAULT_ARTICLE_LIMIT = 10

# --- ì‹¤íŒ¨/ëˆ„ë½ ì¶”ì ìš© ---
failed_college_extractions = []
departments_with_no_results = []
unknown_template_urls = set()
processed_hashes_global = set()

# --- 2. í—¬í¼ í•¨ìˆ˜ ---
def get_soup(url):
    """URL ìš”ì²­ ë° BeautifulSoup ê°ì²´ ë°˜í™˜"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        return BeautifulSoup(response.text, 'html.parser')
    except requests.exceptions.Timeout:
        print(f"      âŒ Timeout ì—ëŸ¬: {url}")
        return None
    except requests.exceptions.RequestException as e:
        print(f"      âŒ ìš”ì²­ ì—ëŸ¬: {url} - {e}")
        return None
    except Exception as e:
        print(f"      âŒ íŒŒì‹± ì—ëŸ¬: {url} - {e}")
        return None

def normalize_text(text):
    """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
    if not text:
        return ""
    text = text.lower().strip()
    return ' '.join(text.split())

def calculate_hash(text):
    """í…ìŠ¤íŠ¸ SHA-256 í•´ì‹œ ê³„ì‚°"""
    if not text:
        return ""
    return hashlib.sha256(text.encode('utf-8')).hexdigest()

def download_image(img_url, base_post_url, save_folder):
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° íŒŒì¼ëª… ë°˜í™˜"""
    if not img_url or img_url.startswith('data:image') or not base_post_url :
        return None
    try:
        absolute_img_url = urljoin(base_post_url, img_url)
        if not absolute_img_url.startswith('http'):
            return None
        absolute_img_url = requests.utils.requote_uri(absolute_img_url)

        img_response = requests.get(absolute_img_url, headers=HEADERS, timeout=20, stream=True)
        img_response.raise_for_status()

        try:
            url_path = urlparse(absolute_img_url).path
            img_filename_base = os.path.basename(url_path) if url_path else None
        except Exception:
             img_filename_base = None

        if not img_filename_base:
             img_filename_base = hashlib.md5(absolute_img_url.encode()).hexdigest()

        valid_chars = "-_.() abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789ê°€-í£"
        img_filename_base = ''.join(c for c in img_filename_base if c in valid_chars).strip()[:100]
        if not img_filename_base:
            img_filename_base = "image"

        content_type = img_response.headers.get('Content-Type')
        ext = '.jpg'
        if content_type:
            content_type = content_type.lower()
            if 'jpeg' in content_type: ext = '.jpg'
            elif 'png' in content_type: ext = '.png'
            elif 'gif' in content_type: ext = '.gif'
            elif 'bmp' in content_type: ext = '.bmp'
            elif 'webp' in content_type: ext = '.webp'
            base, original_ext = os.path.splitext(img_filename_base)
            if original_ext and len(original_ext) <= 5 and original_ext[1:].isalnum():
                 ext = original_ext.lower()
                 img_filename_base = base

        timestamp = int(time.time() * 1000)
        final_filename = f"{img_filename_base}_{timestamp}{ext}"
        save_path = os.path.join(save_folder, final_filename)
        with open(save_path, 'wb') as f:
            for chunk in img_response.iter_content(1024):
                f.write(chunk)
        return final_filename
    except:
        pass
    return None

# --- 3. URL ìë™ ì¶”ì¶œ í•¨ìˆ˜ ---
def extract_notice_board_urls(college_pages_list):
    """ë‹¨ê³¼ëŒ€í•™ ì†Œê°œ í˜ì´ì§€ì—ì„œ í•™ê³¼ë³„ ê³µì§€ì‚¬í•­ URL ìë™ ì¶”ì¶œ"""
    global failed_college_extractions
    print("\n===== 1ë‹¨ê³„: í•™ê³¼ë³„ ê³µì§€ì‚¬í•­ URL ìë™ ì¶”ì¶œ ì‹œì‘ =====")
    department_boards_dict = {}
    base_wwwk_url = "https://wwwk.kangwon.ac.kr"
    dept_block_selector = "div.box.temp_titbox"
    dept_name_selector = "h4.h0"
    notice_link_selector = "ul.shortcut li:last-child a"

    for college in tqdm(college_pages_list, desc="ë‹¨ê³¼ëŒ€í•™ í˜ì´ì§€ ì²˜ë¦¬ ì¤‘"):
        college_name = college['college_name']
        page_url = college['url']
        soup = get_soup(page_url)
        if not soup:
            failed_college_extractions.append(f"{college_name}(ë¡œë“œì‹¤íŒ¨)")
            continue

        dept_blocks = soup.select(dept_block_selector)
        if not dept_blocks:
            failed_college_extractions.append(f"{college_name}(ë¸”ë¡({dept_block_selector}) ì—†ìŒ)")
            continue

        for block in dept_blocks:
            dept_name_element = block.select_one(dept_name_selector)
            notice_link = block.select_one(notice_link_selector)
            if dept_name_element and notice_link:
                dept_name_raw = dept_name_element.get_text(strip=True)
                dept_name_cleaned = dept_name_raw.split('\n')[0].strip()
                relative_or_absolute_url = notice_link.get('href')
                if relative_or_absolute_url:
                    absolute_url = urljoin(base_wwwk_url, relative_or_absolute_url)
                    absolute_url = absolute_url.replace("wwwk.kangwon.ac.kr/wwwk.kangwon.ac.kr", "wwwk.kangwon.ac.kr")
                    if dept_name_cleaned not in department_boards_dict:
                        department_boards_dict[dept_name_cleaned] = absolute_url
        time.sleep(0.1)

    print(f"\n===== URL ìë™ ì¶”ì¶œ ì™„ë£Œ: ì´ {len(department_boards_dict)}ê°œ í•™ê³¼/ì „ê³µ URL í™•ë³´ =====")
    if not department_boards_dict:
        print("ğŸš¨ ìë™ ì¶”ì¶œëœ URLì´ ì—†ìŠµë‹ˆë‹¤!")
        return None
    print("--- ì¶”ì¶œëœ URL ëª©ë¡ (ì¼ë¶€) ---")
    count = 0
    for name, url in department_boards_dict.items():
        print(f"  '{name}': '{url}'")
        count += 1
        if count >= 5:
            print("  ...")
            break
    print("--------------------------")
    return department_boards_dict

# --- 4. í•µì‹¬ í¬ë¡¤ë§ í•¨ìˆ˜ ---
def crawl_post_detail(post_url):
    """ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ (í…œí”Œë¦¿ D ì¶”ê°€ ë° ì„ íƒì ìˆ˜ì •)"""
    global unknown_template_urls
    soup = get_soup(post_url)
    if not soup:
        return None
    title = "ì œëª© ì—†ìŒ"; body_raw = ""; body_hash = ""; image_filenames = []; content_element = None
    title_element = None; detected_template = "Unknown"
    try:
        content_element_A = soup.select_one("#bbs_ntt_cn_con")
        content_element_B = soup.select_one("div.view-comm-board")
        content_element_C = soup.select_one("div.view-content")
        content_element_D = soup.select_one("div.fr-view")

        if content_element_D:
            detected_template = "D"
            title_element = soup.select_one("p.b-title-box span")
            if not title_element: title_element = soup.select_one("div.view_title span")
            content_element = content_element_D
        elif content_element_A:
            detected_template = "A"; title_selector = "div.view_title > span.subject_01"; title_fallback_selector = "div.view_title > span"; content_element = content_element_A
            title_element = soup.select_one(title_selector)
            if not title_element: title_element = soup.select_one(title_fallback_selector)
        elif content_element_B:
            detected_template = "B"; title_selector = "div.view_title > span"; content_element = content_element_B
            title_element = soup.select_one(title_selector)
        elif content_element_C:
            detected_template = "C"; title_selector = "div.view-titbox > p.tit > span"; title_fallback_selector = "div.view-titbox > p.tit"; content_element = content_element_C
            title_element = soup.select_one(title_selector)
            if not title_element: title_element = soup.select_one(title_fallback_selector)
        else:
            unknown_template_urls.add(post_url)
            title_selectors_fallback = ["h1", "h2", "h3", "h4", ".title", "#title", ".view_title", ".subject", "td.td_subject", "p.b-title-box span"]
            content_selectors_fallback = ["#article_content", "#bbs_content", "#view_content", ".content", ".view_content", ".view_cont", ".dbdata", "article", "section", "td.td_content", "div.fr-view"]
            for ts in title_selectors_fallback:
                title_element = soup.select_one(ts)
                if title_element: break
            for cs in content_selectors_fallback:
                content_element = soup.select_one(cs)
                if content_element: break

        title = title_element.get_text(strip=True) if title_element else "ì œëª© ì—†ìŒ"
        if content_element:
            for unwanted in content_element.select('.reply_area, .related_posts'):
                 unwanted.decompose()
            body_raw = content_element.get_text(separator='\n', strip=True)
        else:
             body_raw = ""

        body_normalized = normalize_text(body_raw)
        body_hash = calculate_hash(body_normalized)
        image_filenames = []
        if content_element:
            img_tags = content_element.select("img")
            for img in img_tags:
                img_src = img.get('src')
                if img_src:
                    saved_filename = download_image(img_src, post_url, IMAGE_FOLDER)
                    if saved_filename:
                        image_filenames.append(saved_filename)

        return {'ì œëª©': title, 'ë³¸ë¬¸': body_raw, 'í•´ì‹œ': body_hash, 'ì´ë¯¸ì§€íŒŒì¼ëª…': ", ".join(image_filenames)}

    except Exception as e:
        print(f"      âŒ ìƒì„¸ ì²˜ë¦¬ ì˜ˆì™¸ ({post_url}): {e}")
        traceback.print_exc()
        return None

def build_page_url(base_url, page_num, articles_per_page=DEFAULT_ARTICLE_LIMIT):
    """ê¸°ë³¸ URL, í˜ì´ì§€ ë²ˆí˜¸, í˜ì´ì§€ë‹¹ ê²Œì‹œë¬¼ ìˆ˜ë¥¼ ë°›ì•„ URL (offset ì‚¬ìš©)ì„ ìƒì„±"""
    try:
        parsed_url = urlparse(base_url)
        query_params = parse_qs(parsed_url.query, keep_blank_values=True)
        offset = (page_num - 1) * articles_per_page
        query_params['article.offset'] = [str(offset)]
        if 'articleLimit' not in query_params:
             query_params['articleLimit'] = [str(articles_per_page)]
        query_params.pop('mode', None)
        query_params.pop('articleNo', None)
        query_params.pop('pageIndex', None)
        new_query = urlencode(query_params, doseq=True)
        new_url_parts = parsed_url._replace(query=new_query)
        return urlunparse(new_url_parts)
    except Exception as e:
        print(f"      âš ï¸ URL ìƒì„± ì˜¤ë¥˜ (Base: {base_url}, Page: {page_num}): {e}")
        return None

def crawl_board_list(dept_name, board_url):
    """í•™ê³¼ ê²Œì‹œíŒì˜ ëª¨ë“  í˜ì´ì§€ë¥¼ í¬ë¡¤ë§ (Offset ê¸°ë°˜ ì´ í˜ì´ì§€ ê³„ì‚°, ì„ íƒì ìˆ˜ì •)"""
    all_posts_data = []
    processed_hashes_in_dept = set()

    print(f"--- [{dept_name}] ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘ ---")
    print(f"  ì‹œì‘ URL: {board_url}")

    print(f"  -> [{dept_name}] ì²« í˜ì´ì§€ ë¡œë“œ ì‹œë„ (ì´ í˜ì´ì§€ í™•ì¸ìš©)...")
    initial_soup = get_soup(board_url)
    if not initial_soup:
        print(f"      âŒ [{dept_name}] ì²« í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨. í•´ë‹¹ í•™ê³¼ ê±´ë„ˆ<0xEB><0x9B><0x81>ë‹ˆë‹¤.")
        departments_with_no_results.append(dept_name + "(ì²« í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨)")
        return []

    totalPages = 1
    articles_per_page = DEFAULT_ARTICLE_LIMIT
    determined_total_pages = False
    try:
        # í˜ì´ì§€ë„¤ì´ì…˜ ì˜ì—­ ì°¾ê¸°
        pagination_wrap = initial_soup.select_one('div.b-paging-wrap, div.paginate')
        if pagination_wrap:
            # "ë§¨ë" ë²„íŠ¼ì˜ href ì†ì„±ì—ì„œ offset ì°¾ê¸°
            last_page_link_href = pagination_wrap.select_one('li.last a[href]')
            if not last_page_link_href: last_page_link_href = pagination_wrap.select_one('a.last[href]')
            if not last_page_link_href: last_page_link_href = pagination_wrap.select_one('a.next_end[href]')

            if last_page_link_href:
                href = last_page_link_href.get('href')
                parsed_href = urlparse(href)
                query_params = parse_qs(parsed_href.query)

                # articleLimit ê°’ ì¶”ì¶œ
                temp_limit = articles_per_page
                if 'articleLimit' in query_params:
                    try:
                        temp_limit = int(query_params['articleLimit'][0])
                    except (ValueError, IndexError):
                        pass
                else:
                    any_page_link = pagination_wrap.select_one('a[href*="articleLimit="]')
                    if any_page_link:
                        parsed_any = urlparse(any_page_link.get('href'))
                        query_any = parse_qs(parsed_any.query)
                        if 'articleLimit' in query_any:
                           try:
                               temp_limit = int(query_any['articleLimit'][0])
                           except(ValueError, IndexError):
                               pass
                articles_per_page = temp_limit

                # article.offset ê°’ ì¶”ì¶œ ë° ì´ í˜ì´ì§€ ê³„ì‚°
                if 'article.offset' in query_params:
                    try:
                        last_offset = int(query_params['article.offset'][0])
                        if articles_per_page > 0:
                            totalPages = (last_offset // articles_per_page) + 1
                            print(f"      â„¹ï¸ 'ë§¨ë' ë²„íŠ¼ href offsetì—ì„œ ì´ í˜ì´ì§€ ìˆ˜ ê³„ì‚°: {totalPages} (offset={last_offset}, limit={articles_per_page})")
                            determined_total_pages = True
                        else:
                             print(f"      âš ï¸ articles_per_pageê°€ 0ì´ì–´ì„œ ì´ í˜ì´ì§€ ê³„ì‚° ë¶ˆê°€.")
                    except (ValueError, IndexError):
                        print(f"      âš ï¸ 'ë§¨ë' ë²„íŠ¼ href offset íŒŒì‹± ì‹¤íŒ¨: {href}")
                else:
                    print(f"      âš ï¸ 'ë§¨ë' ë²„íŠ¼ hrefì— article.offset ì—†ìŒ: {href}")
            else:
                 print("      â„¹ï¸ Offset ë°©ì‹ì˜ 'ë§¨ë' ë²„íŠ¼(li.last a, a.last, a.next_end) ì—†ìŒ.")

        if not determined_total_pages:
            print("      â„¹ï¸ ì´ í˜ì´ì§€ ìˆ˜ë¥¼ ëª…í™•íˆ ì•Œ ìˆ˜ ì—†ì–´ ê¸°ë³¸ 1í˜ì´ì§€ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤. (Offset ë°©ì‹ ì‹¤íŒ¨)")
            totalPages = 1
        elif totalPages <= 0 :
             print(f"      âš ï¸ ê³„ì‚°ëœ ì´ í˜ì´ì§€ ìˆ˜ê°€ 0 ì´í•˜({totalPages})ì…ë‹ˆë‹¤. 1í˜ì´ì§€ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
             totalPages = 1

    except Exception as e:
        print(f"      âš ï¸ ì´ í˜ì´ì§€ ìˆ˜ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ê¸°ë³¸ 1í˜ì´ì§€ë§Œ ì²˜ë¦¬.")
        totalPages = 1
        traceback.print_exc()

    # í˜ì´ì§€ ë£¨í”„
    for page_num in range(1, totalPages + 1):
        if page_num == 1:
            page_url = board_url
            soup = initial_soup
        else:
            page_url = build_page_url(board_url, page_num, articles_per_page)
            if not page_url:
                 print(f"      âŒ [{dept_name}] í˜ì´ì§€ {page_num} URL ìƒì„± ì‹¤íŒ¨. ê±´ë„ˆ<0xEB><0x9B><0x81>ë‹ˆë‹¤.")
                 continue
            print(f"  -> [{dept_name}] í˜ì´ì§€ {page_num}/{totalPages} í¬ë¡¤ë§ ì¤‘: {page_url}")
            soup = get_soup(page_url)
            if not soup:
                print(f"      âŒ [{dept_name}] í˜ì´ì§€ {page_num} ë¡œë“œ ì‹¤íŒ¨. ê±´ë„ˆ<0xEB><0x9B><0x81>ë‹ˆë‹¤.")
                continue

        # í˜ì´ì§€ ë‚´ ê²Œì‹œê¸€ ì²˜ë¦¬
        post_rows_selector = "tbody > tr"
        post_rows = soup.select(post_rows_selector)
        if not post_rows and page_num == 1:
            print(f"      âš ï¸ [{dept_name}] í˜ì´ì§€ {page_num}: ê²Œì‹œê¸€ í–‰({post_rows_selector}) ì—†ìŒ.")

        found_posts_on_page = 0
        for index, row in enumerate(post_rows):
            cells = row.select("td")
            is_sticky = False
            if len(cells) < 2:
                continue
            try:
                first_cell_content = cells[0]
                is_sticky_img = first_cell_content.find('img', alt=lambda x: x and 'ê³µì§€' in x) is not None
                first_cell_text = first_cell_content.get_text(strip=True)
                is_sticky_text = not first_cell_text.isdigit() if first_cell_text else False
                is_sticky = is_sticky_img or is_sticky_text
            except IndexError:
                continue
            if is_sticky and page_num > 1:
                continue

            # ë‚ ì§œ ì¶”ì¶œ (ìˆ˜ì •ë¨)
            date_from_list = "ë‚ ì§œ ì—†ìŒ"
            date_cell = row.select_one("td:nth-of-type(4)") # 4ë²ˆì§¸ tdë§Œ í™•ì¸
            if date_cell:
                date_text = date_cell.get_text(strip=True)
                match = re.search(r'\d{4}[-./]\d{2}[-./]\d{2}', date_text)
                if match:
                    date_from_list = match.group().replace('-', '.').replace('/', '.')

            # ìƒì„¸ URL ì¶”ì¶œ
            title_element = None
            title_cell_candidates = ["td:nth-of-type(2) a[href]", "td.title a[href]", "td.subject a[href]"]
            for selector in title_cell_candidates:
                 title_element = row.select_one(selector)
                 if title_element:
                     break
            if not title_element:
                continue

            post_relative_url = title_element.get('href')
            if not post_relative_url or post_relative_url.startswith('javascript:'):
                continue
            post_absolute_url = urljoin(page_url, post_relative_url)

            # ìƒì„¸ í¬ë¡¤ë§ ë° ë°ì´í„° ì²˜ë¦¬
            detail_data = crawl_post_detail(post_absolute_url) # ìˆ˜ì •ëœ ìƒì„¸ í•¨ìˆ˜ í˜¸ì¶œ
            if not detail_data or not detail_data.get('í•´ì‹œ'):
                continue
            post_hash = detail_data.get('í•´ì‹œ')
            if post_hash in processed_hashes_in_dept:
                continue
            processed_hashes_in_dept.add(post_hash)

            final_data = {
                'í•™ê³¼': dept_name,
                'ì‘ì„±ì¼': date_from_list, # ìˆ˜ì •ëœ ë‚ ì§œ
                'ì œëª©': detail_data.get('ì œëª©', 'ì œëª© ì—†ìŒ'), # ìˆ˜ì •ëœ ì œëª©
                'ë³¸ë¬¸': detail_data.get('ë³¸ë¬¸', ''), # ìˆ˜ì •ëœ ë³¸ë¬¸
                'í•´ì‹œ': post_hash,
                'ì›ë³¸URL': post_absolute_url,
                'ì´ë¯¸ì§€íŒŒì¼ëª…': detail_data.get('ì´ë¯¸ì§€íŒŒì¼ëª…', '')
            }
            if final_data['ì œëª©'] != "ì œëª© ì—†ìŒ":
                 all_posts_data.append(final_data)
                 found_posts_on_page += 1

        print(f"      -> í˜ì´ì§€ {page_num}: {found_posts_on_page}ê°œ ì‹ ê·œ ê²Œì‹œê¸€ ì²˜ë¦¬ ì™„ë£Œ.")
        if totalPages > 1 and page_num < totalPages :
             time.sleep(REQUEST_DELAY * 1.5) # í˜ì´ì§€ ì´ë™ ë”œë ˆì´

    print(f"--- [{dept_name}] ê²Œì‹œíŒ ì²˜ë¦¬ ì™„ë£Œ: ì´ {len(all_posts_data)}ê°œ ìœ íš¨ ë°ì´í„° ìˆ˜ì§‘ ({totalPages} í˜ì´ì§€ í™•ì¸) ---")
    if not all_posts_data:
        departments_with_no_results.append(dept_name + f"({totalPages}p í™•ì¸,ê²°ê³¼ì—†ìŒ)")
    return all_posts_data

# --- 5. ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
if __name__ == "__main__":
    start_time_total = time.time()
    department_boards_result = extract_notice_board_urls(college_intro_pages)
    all_results_before_dedup = []
    df_final_output = pd.DataFrame()
    if not department_boards_result:
        print("\nğŸš¨ URL ìë™ ì¶”ì¶œ ì‹¤íŒ¨.")
    else:
        print("\n===== 2ë‹¨ê³„: ê²Œì‹œíŒë³„ *ì „ì²´* í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘ =====")
        start_time_crawl = time.time()
        print(f"ì´ {len(department_boards_result)}ê°œ í•™ê³¼/ì „ê³µ í¬ë¡¤ë§...")
        for dept_name, board_url in tqdm(department_boards_result.items(), desc="ì „ì²´ í•™ê³¼ ì§„í–‰ë¥ "):
            results = crawl_board_list(dept_name, board_url) # ìˆ˜ì •ëœ í•¨ìˆ˜ í˜¸ì¶œ
            if results:
                all_results_before_dedup.extend(results)
        crawl_end_time = time.time()
        print("\n===== í¬ë¡¤ë§ ì™„ë£Œ (ì¤‘ë³µ ì œê±° ì „) =====")
        print(f"ì´ {len(all_results_before_dedup)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ.")
        print(f"í¬ë¡¤ë§ ì†Œìš” ì‹œê°„: {crawl_end_time - start_time_crawl:.2f} ì´ˆ")

        if all_results_before_dedup:
            try: # --- 3ë‹¨ê³„: ì¤‘ë³µ ì œê±° ë° CSV ì €ì¥ ---
                df = pd.DataFrame(all_results_before_dedup)
                print("\n--- 3ë‹¨ê³„: ì¤‘ë³µ ì œê±° ì‹œì‘ ---")
                initial_count = len(df)
                df_deduplicated = pd.DataFrame()
                if 'í•´ì‹œ' in df.columns and 'ì œëª©' in df.columns:
                    df_deduplicated = df.dropna(subset=['ì œëª©', 'í•´ì‹œ']).drop_duplicates(subset=['ì œëª©', 'í•´ì‹œ'], keep='first')
                else:
                    print("âš ï¸ ì»¬ëŸ¼ ë¶€ì¡±. ì œëª© ê¸°ì¤€ ì¤‘ë³µ ì œê±° ì‹œë„.")
                    df_deduplicated = df.dropna(subset=['ì œëª©']).drop_duplicates(subset=['ì œëª©'], keep='first')
                removed_count = initial_count - len(df_deduplicated)
                print(f"ì¤‘ë³µ ì œê±° í›„ {len(df_deduplicated)}ê°œ ê³ ìœ  ê²Œì‹œê¸€ ë‚¨ìŒ. ({removed_count}ê°œ ì œê±°ë¨)")

                # CSV ì €ì¥ ì»¬ëŸ¼ (ê³µì§€ê¸€ ì—´ ì œì™¸)
                final_columns_map = {'í•™ê³¼': 'í•™ê³¼', 'ì œëª©': 'ì œëª©', 'ì‘ì„±ì¼': 'ì‘ì„±ì¼', 'ë³¸ë¬¸': 'ë³¸ë¬¸ë‚´ìš©','ì›ë³¸URL': 'URL', 'ì´ë¯¸ì§€íŒŒì¼ëª…': 'ì´ë¯¸ì§€'}
                df_to_save = df_deduplicated[[col for col in final_columns_map.keys() if col in df_deduplicated.columns]].rename(columns=final_columns_map)
                # CSV ì €ì¥ ìˆœì„œ (ê³µì§€ê¸€ ì—´ ì œì™¸)
                desired_order = ['í•™ê³¼', 'ì œëª©', 'ì‘ì„±ì¼', 'ë³¸ë¬¸ë‚´ìš©', 'URL', 'ì´ë¯¸ì§€']
                df_final_output = df_to_save[[col for col in desired_order if col in df_to_save.columns]]

                df_final_output.to_csv(CSV_FILEPATH, index=False, encoding='utf-8-sig')
                print(f"\nâœ… ìµœì¢… ê²°ê³¼ë¥¼ '{CSV_FILEPATH}'ì— ì €ì¥.")
                print(f"  ì €ì¥ëœ ì»¬ëŸ¼: {list(df_final_output.columns)}")
            except ImportError:
                print("\nâŒ 'pandas' í•„ìš”")
                df_final_output = pd.DataFrame()
            except Exception as e:
                print(f"\nâŒ CSV ì €ì¥/ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                traceback.print_exc()
                df_final_output = pd.DataFrame()
        else:
             print("\nâš ï¸ ì €ì¥í•  ìœ íš¨ ë°ì´í„° ì—†ìŒ.")

    # --- 4ë‹¨ê³„: ì´ë¯¸ì§€ íŒŒì¼ ì •ë¦¬ ---
    print("\n===== 4ë‹¨ê³„: ì´ë¯¸ì§€ íŒŒì¼ ì •ë¦¬ ì‹œì‘ =====")
    try:
        if not df_final_output.empty and os.path.exists(CSV_FILEPATH):
            print(f"'{CSV_FILEPATH}' ê¸°ì¤€ ì´ë¯¸ì§€ ì •ë¦¬...")
            if 'ì´ë¯¸ì§€' not in df_final_output.columns:
                print("âš ï¸ 'ì´ë¯¸ì§€' ì»¬ëŸ¼ ì—†ìŒ.")
            else:
                referenced_images = set()
                for image_cell in df_final_output['ì´ë¯¸ì§€'].dropna():
                    if isinstance(image_cell, str):
                        filenames = [img.strip() for img in image_cell.split(',') if img.strip()]
                        referenced_images.update(filenames)
                print(f"-> ìµœì¢… ì°¸ì¡° ì´ë¯¸ì§€ {len(referenced_images)}ê°œ í™•ì¸.")
                try:
                    if not os.path.isdir(IMAGE_FOLDER):
                         print(f"âš ï¸ ì´ë¯¸ì§€ í´ë” '{IMAGE_FOLDER}' ì—†ìŒ.")
                    else:
                        actual_files_in_folder = [f for f in os.listdir(IMAGE_FOLDER) if os.path.isfile(os.path.join(IMAGE_FOLDER, f))]
                        print(f"-> í´ë” ë‚´ íŒŒì¼ {len(actual_files_in_folder)}ê°œ í™•ì¸.")
                        files_to_delete = [f for f in actual_files_in_folder if f not in referenced_images]
                        if not files_to_delete:
                            print("âœ… ì‚­ì œí•  ë¶ˆí•„ìš” ì´ë¯¸ì§€ ì—†ìŒ.")
                        else:
                            print(f"-> ì‚­ì œ ì˜ˆì •: {len(files_to_delete)}ê°œ")
                            if files_to_delete:
                                deleted_count = 0
                                error_count = 0
                                print("... ì´ë¯¸ì§€ ì‚­ì œ ì‘ì—… ì§„í–‰ ì¤‘ ...")
                                for filename in tqdm(files_to_delete, desc="ë¶ˆí•„ìš” ì´ë¯¸ì§€ ì‚­ì œ ì¤‘"):
                                    try:
                                        os.remove(os.path.join(IMAGE_FOLDER, filename))
                                        deleted_count += 1
                                    except OSError as e:
                                        print(f"\n  âŒ ì‚­ì œ ì‹¤íŒ¨: {filename} - {e}")
                                        error_count += 1
                                print(f"\nâœ… ì •ë¦¬ ì™„ë£Œ: {deleted_count}ê°œ ì‚­ì œ, {error_count}ê°œ ì‹¤íŒ¨.")
                except Exception as e:
                    print(f"âŒ í´ë” ì¡°íšŒ ì˜¤ë¥˜: {e}")
        elif df_final_output.empty:
             print("â„¹ï¸ ìµœì¢… ë°ì´í„° ì—†ìŒ.")
        else:
             print(f"â„¹ï¸ CSV íŒŒì¼ '{CSV_FILEPATH}' ì—†ìŒ.")
    except Exception as e:
        print(f"\nâŒ ì´ë¯¸ì§€ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()

    # --- ìµœì¢… ìš”ì•½ ë° ëˆ„ë½ ì •ë³´ ì¶œë ¥ ---
    print("\n===== í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ =====")
    final_post_count = 0
    if not df_final_output.empty:
        final_post_count = len(df_final_output)
    print(f"ì´ {len(department_boards_result) if department_boards_result else 0}ê°œ í•™ê³¼/ì „ê³µ URL ì‹œë„.")
    print(f"ìµœì¢… ìˆ˜ì§‘ëœ ê³ ìœ  ê²Œì‹œê¸€ ìˆ˜: {final_post_count}")
    if failed_college_extractions:
        print("\n[ğŸ”´ 1ë‹¨ê³„ URL ì¶”ì¶œ ì‹¤íŒ¨/ëˆ„ë½]")
        for college in failed_college_extractions:
            print(f"- {college}")

    departments_needing_analysis = set(departments_with_no_results)
    analyzed_dept_names_from_unknown = set()
    if department_boards_result:
        for url in unknown_template_urls:
            found_dept = None
            for name, board_url_from_dict in department_boards_result.items():
                try:
                    base_board_url = board_url_from_dict.split('?')[0]
                    if url.startswith(base_board_url) or url.startswith(board_url_from_dict):
                        found_dept = name
                        break
                except Exception: pass
            if found_dept:
                analyzed_dept_names_from_unknown.add(found_dept)
    departments_needing_analysis.update(analyzed_dept_names_from_unknown)

    if departments_needing_analysis:
        print("\n[ğŸŸ¡ 2ë‹¨ê³„ ìƒì„¸ ë¶„ì„ í•„ìš” í•™ê³¼ ëª©ë¡]")
        print("  (ì›ì¸: ì²« í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨, ê²Œì‹œê¸€ ì—†ìŒ, í…œí”Œë¦¿ ë¯¸ì¸ì‹ ë“±)")
        for dept in sorted(list(departments_needing_analysis)):
            print(f"- {dept}")
        if unknown_template_urls:
            print("\n  ë¯¸ì¸ì‹ ìƒì„¸ í˜ì´ì§€ URL (ì¼ë¶€):")
            for i, url in enumerate(list(unknown_template_urls)):
                 if i >= 5: print("  ..."); break
                 print(f"  - {url}")
        print("\n  => ìœ„ í•™ê³¼ HTML êµ¬ì¡° ë¶„ì„ ë° ì„ íƒì ìˆ˜ì • í•„ìš”.")
    elif unknown_template_urls:
         print("\n[âš ï¸ ìƒì„¸ í˜ì´ì§€ í…œí”Œë¦¿ ë¯¸ì¸ì‹ URL ëª©ë¡]")
         for i, url in enumerate(list(unknown_template_urls)):
             if i >= 10: print(" ..."); break
             print(f"- {url}")

    # --- ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œê°„ ì¸¡ì • ---
    total_end_time = time.time()
    print(f"\n===== ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ (ì´ ì‹œê°„: {total_end_time - start_time_total:.2f} ì´ˆ) =====")
    