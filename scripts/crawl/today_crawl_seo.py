import os
import csv
import time
import re
import random
import unicodedata
from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse
import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from tqdm import tqdm
from difflib import SequenceMatcher
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from datetime import datetime, timedelta

# --- ê¸°ë³¸ ì„¤ì • ---
HEADERS = {"User-Agent": "Mozilla/5.0"}
SAVE_FOLDER = "ê°•ì›ëŒ€ ì „ ê¸°ê´€ + ì „ í•™ê³¼_ì´ë¯¸ì§€"
CSV_FILE = "ê°•ì›ëŒ€ í†µí•© ê³µì§€ì‚¬í•­ í¬ë¡¤ë§.csv"
os.makedirs(SAVE_FOLDER, exist_ok=True)

# â–¼â–¼â–¼ í¬ë¡¤ë§ ì‹œì‘ ë‚ ì§œ ì„¤ì • â–¼â–¼â–¼
CRAWL_START_DATE = (datetime.now() - timedelta(days=3)).strftime("%Y-%m-%d")
# â–²â–²â–² ì„¤ì • ì™„ë£Œ â–²â–²â–²

# --- ì„¸ì…˜ ë° ì¬ì‹œë„ ì„¤ì • ---
session = requests.Session()
retry = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
adapter = HTTPAdapter(max_retries=retry)
session.mount("http://", adapter)
session.mount("https://", adapter)

# --- ì „ì—­ ë³€ìˆ˜ ---
all_data = []
pre_existing_data = [] # ê¸°ì¡´ CSV ë°ì´í„°ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---
def sanitize_filename(name):
    return re.sub(r'[^\w\s_.-]', '_', name).strip()

def save_image(img_url, folder, prefix, idx, original_name="image.jpg"):
    try:
        os.makedirs(folder, exist_ok=True)
        ext = os.path.splitext(original_name)[1]
        if not ext or len(ext) > 5: ext = '.jpg'
        filename = sanitize_filename(f"{prefix}_{idx}{ext}")
        filepath = os.path.join(folder, filename)
        res = session.get(img_url, headers=HEADERS, timeout=10)
        time.sleep(random.uniform(0.1, 0.3))
        if res.status_code == 200 and len(res.content) > 1024:
            with open(filepath, "wb") as f:
                f.write(res.content)
            return filepath.replace("\\", "/")
    except Exception as e:
        print(f"       âš ï¸ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {img_url} ({e})")
    return None

def clean_html_keep_table(raw_html):
    soup = BeautifulSoup(raw_html, 'html.parser')
    # â–¼â–¼â–¼â–¼â–¼ ì¶”ê°€ëœ ë¶€ë¶„ â–¼â–¼â–¼â–¼â–¼
    # 'ì‚¬ì§„ í™•ëŒ€ë³´ê¸°'ë¥¼ í¬í•¨í•œ span íƒœê·¸ë¥¼ ì°¾ì•„ì„œ ì œê±°
    for zoom_element in soup.select('span.photo_zoom'):
        zoom_element.decompose()
    # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
    output = []
    for table in soup.find_all('table'):
        output.append(extract_table_text(table))
        table.decompose()
    for br in soup.find_all("br"):
        br.replace_with("\n")
    for elem in soup.find_all(['p', 'div', 'span']):
        text = elem.get_text(strip=True, separator="\n")
        if text and text not in output:
            output.append(text)
    return "\n".join(output).strip()

def extract_table_text(table):
    rows = table.find_all('tr')
    return '\n'.join(
        ' | '.join(col.get_text(strip=True) for col in row.find_all(['td', 'th']) if col.get_text(strip=True))
        for row in rows if row.find_all(['td', 'th'])
    )

def extract_written_date(soup):
    text = soup.get_text(" ", strip=True)
    match = re.search(r'20\d{2}[.\-/ë…„\s]+[01]?\d[.\-/ì›”\s]+[0-3]?\d[ì¼\s]*', text)
    if match:
        raw = match.group().replace(" ", "").replace("ë…„", ".").replace("ì›”", ".").replace("ì¼", "")
        return raw.strip(".")
    return None

def generate_notice_key(title, date):
    """
    ì œëª©ê³¼ ë‚ ì§œë¥¼ ë°›ì•„ í‘œì¤€í™”ëœ í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    - ì œëª©ì€ ì†Œë¬¸ìí™” ë° ìœ ë‹ˆì½”ë“œ ì •ê·œí™”ë©ë‹ˆë‹¤.
    - íŠ¹ìˆ˜ë¬¸ìëŠ” ì œê±°í•˜ë˜, ë‹¨ì–´ êµ¬ë¶„ì„ ìœ„í•œ ë‹¨ì¼ ê³µë°±ì€ ìœ ì§€ë©ë‹ˆë‹¤.
    - ë‚ ì§œëŠ” êµ¬ë¶„ìê°€ ëª¨ë‘ ì œê±°ëœ ìˆ«ì í˜•ì‹ìœ¼ë¡œ ë°”ë€ë‹ˆë‹¤.
    """
    temp_title = title if title else ""
    date_str = date if date else ""

    # 1. ìœ ë‹ˆì½”ë“œ ì •ê·œí™” ë° ì†Œë¬¸ì ë³€í™˜
    processed_title = unicodedata.normalize('NFKC', temp_title.lower())
    
    # 2. íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì•ŒíŒŒë²³, ìˆ«ì, í•œê¸€, ê³µë°±ë§Œ ë‚¨ê¹€)
    # ê´„í˜¸ ì œê±° ë¡œì§ì„ ì‚­ì œí•˜ê³ , ë„ì–´ì“°ê¸°ë¥¼ ë³´ì¡´í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½
    processed_title = re.sub(r'[^\w\sê°€-í£]', '', processed_title)
    
    # 3. ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ì •ê·œí™”
    processed_title = ' '.join(processed_title.split())

    # 4. ë‚ ì§œ í˜•ì‹ í‘œì¤€í™” (ê¸°ì¡´ê³¼ ë™ì¼)
    date_str = date_str.replace('.', '').replace('-', '').replace('/', '').replace(' ', '').lower()
    
    return f"{processed_title}_{date_str}"

def get_soup(url):
    try:
        response = session.get(url, headers=HEADERS, timeout=15)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        return BeautifulSoup(response.text, 'html.parser')
    except Exception as e:
        print(f"       âŒ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {url} ({e})")
        return None

def delay_request(min_sec=0.5, max_sec=1.5):
    time.sleep(random.uniform(min_sec, max_sec))

def is_too_old(post_date_str, start_date_obj, date_format="%Y.%m.%d"):
    """ê²Œì‹œë¬¼ ë‚ ì§œê°€ ì§€ì •ëœ ì‹œì‘ ë‚ ì§œë³´ë‹¤ ì˜¤ë˜ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
    if not post_date_str:
        return False
    try:
        post_date_obj = datetime.strptime(post_date_str, date_format)
        return post_date_obj < start_date_obj
    except (ValueError, TypeError):
        return False

# --- í•µì‹¬ ë¡œì§: ë°ì´í„° ë¡œë“œ ë° ì¶”ê°€ ---
def load_existing_data():
    """ì‹œì‘ ì‹œ CSV íŒŒì¼ì„ ì½ì–´ ëª¨ë“  ë°ì´í„°ë¥¼ ì „ì—­ ë¦¬ìŠ¤íŠ¸(pre_existing_data)ì— ë¡œë“œí•©ë‹ˆë‹¤."""
    global pre_existing_data
    if not os.path.exists(CSV_FILE):
        print("ğŸ“„ ê¸°ì¡´ CSV íŒŒì¼ì´ ì—†ì–´ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        return
    try:
        print(f"ğŸ“„ '{CSV_FILE}' íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        with open(CSV_FILE, "r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f)
            if "ì œëª©" not in reader.fieldnames or "ì‘ì„±ì¼" not in reader.fieldnames:
                print(f"âš ï¸ '{CSV_FILE}'ì— 'ì œëª©' ë˜ëŠ” 'ì‘ì„±ì¼' ì»¬ëŸ¼ì´ ì—†ì–´ ì¤‘ë³µ ê²€ì‚¬ë¥¼ ìœ„í•œ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            pre_existing_data = list(reader)
            print(f"âœ… ê¸°ì¡´ ë°ì´í„° {len(pre_existing_data)}ê±´ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤. ìƒˆ ê²Œì‹œë¬¼ì€ ìµœê·¼ 3ì¼ì¹˜ ë°ì´í„°ì™€ ë¹„êµí•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ê¸°ì¡´ CSV íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def add_notice_if_not_duplicate(title, date, content, link, images):
    """
    ìµœì¢… ë¡œì§ì— ë”°ë¼ ì¤‘ë³µì„ ê²€ì‚¬í•˜ê³ , ì¤‘ë³µì´ ì•„ë‹ ê²½ìš°ì—ë§Œ ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    1. í˜„ì¬ ì„¸ì…˜ ë‚´ì—ì„œ ì™„ì „ ì¼ì¹˜ ë° ìœ ì‚¬ë„ ê²€ì‚¬
    2. ê¸°ì¡´ CSV ë°ì´í„° ì¤‘, ìƒˆ ê²Œì‹œë¬¼ ë‚ ì§œ ê¸°ì¤€ -3ì¼ ë²”ìœ„ ë‚´ì—ì„œë§Œ ì™„ì „ ì¼ì¹˜ ë° ìœ ì‚¬ë„ ê²€ì‚¬
    3. ìœ ì‚¬ë„ ê²€ì‚¬ ì¡°ê±´: Sequence Matcher >= 0.9 AND Cosine Similarity >= 0.8
    """
    SEQ_MATCHER_THRESHOLD = 0.9
    COSINE_SIMILARITY_THRESHOLD = 0.8
    DATE_WINDOW_DAYS = 3
    new_key = generate_notice_key(title, date)
    normalized_new_title = generate_notice_key(title, "_").split('_')[0]
    
    vectorizer = TfidfVectorizer()

    # 1. í˜„ì¬ ì„¸ì…˜ ë‚´ì—ì„œ ì¤‘ë³µ ê²€ì‚¬ (ì™„ì „ ì¼ì¹˜ + ìœ ì‚¬ë„)
    try:
        new_date_obj_session = datetime.strptime(date, "%Y.%m.%d")
        for post_in_session in all_data:
            # (A) ì™„ì „ ì¼ì¹˜ ê²€ì‚¬
            if new_key == generate_notice_key(post_in_session['ì œëª©'], post_in_session['ì‘ì„±ì¼']):
                print(f"       ğŸš« [ì¤‘ë³µ-ì„¸ì…˜/ì™„ì „ì¼ì¹˜] {title[:40]}")
                return False
            
            # (B) ìœ ì‚¬ë„ ê²€ì‚¬ (ë‚ ì§œê°€ ë¹„ìŠ·í•œ ê²½ìš°ì—ë§Œ ìˆ˜í–‰)
            try:
                session_date_obj = datetime.strptime(post_in_session['ì‘ì„±ì¼'], "%Y.%m.%d")
                if abs((new_date_obj_session - session_date_obj).days) <= DATE_WINDOW_DAYS:
                    normalized_session_title = generate_notice_key(post_in_session['ì œëª©'], "_").split('_')[0]
                    
                    seq_ratio = SequenceMatcher(None, normalized_new_title, normalized_session_title).ratio()
                    
                    try:
                        tfidf_matrix = vectorizer.fit_transform([normalized_new_title, normalized_session_title])
                        cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
                    except ValueError:
                        cosine_sim = 0 # ë‹¨ì–´ê°€ ì—†ëŠ” ì œëª© ì²˜ë¦¬

                    if seq_ratio >= SEQ_MATCHER_THRESHOLD and cosine_sim >= COSINE_SIMILARITY_THRESHOLD:
                        print(f"       ğŸš« [ì¤‘ë³µ-ì„¸ì…˜/ìœ ì‚¬ë„] (Seq: {seq_ratio:.2f}, Cos: {cosine_sim:.2f}) {title[:40]}")
                        return False
            except (ValueError, TypeError):
                continue
    except (ValueError, TypeError):
        for post_in_session in all_data:
            if new_key == generate_notice_key(post_in_session['ì œëª©'], post_in_session['ì‘ì„±ì¼']):
                print(f"       ğŸš« [ì¤‘ë³µ-ì„¸ì…˜/ì™„ì „ì¼ì¹˜] {title[:40]}")
                return False

    # 2. ê¸°ì¡´ íŒŒì¼ ë°ì´í„°ì™€ ë‚ ì§œ ì°½(Date Window) ë‚´ì—ì„œ ì¤‘ë³µ ê²€ì‚¬ (ì™„ì „ ì¼ì¹˜ + ìœ ì‚¬ë„)
    try:
        new_date_obj = datetime.strptime(date, "%Y.%m.%d")
        start_date_window = new_date_obj - timedelta(days=DATE_WINDOW_DAYS)

        for existing_post in pre_existing_data:
            try:
                existing_date_str = existing_post.get('ì‘ì„±ì¼')
                if not existing_date_str: continue
                
                existing_date_obj = datetime.strptime(existing_date_str, "%Y.%m.%d")

                if start_date_window <= existing_date_obj <= new_date_obj:
                    existing_title = existing_post.get('ì œëª©', '')
                    
                    if new_key == generate_notice_key(existing_title, existing_date_str):
                        print(f"       ğŸš« [ì¤‘ë³µ-ê¸°ì¡´íŒŒì¼/ì™„ì „ì¼ì¹˜] {title[:40]}")
                        return False
                    
                    normalized_existing_title = generate_notice_key(existing_title, "_").split('_')[0]
                    
                    seq_ratio = SequenceMatcher(None, normalized_new_title, normalized_existing_title).ratio()

                    try:
                        tfidf_matrix = vectorizer.fit_transform([normalized_new_title, normalized_existing_title])
                        cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
                    except ValueError:
                        cosine_sim = 0 # ë‹¨ì–´ê°€ ì—†ëŠ” ì œëª© ì²˜ë¦¬

                    if seq_ratio >= SEQ_MATCHER_THRESHOLD and cosine_sim >= COSINE_SIMILARITY_THRESHOLD:
                        print(f"       ğŸš« [ì¤‘ë³µ-ê¸°ì¡´íŒŒì¼/ìœ ì‚¬ë„] (Seq: {seq_ratio:.2f}, Cos: {cosine_sim:.2f}) {title[:40]}")
                        return False
            except (ValueError, TypeError):
                continue
    except (ValueError, TypeError):
        pass

    # ëª¨ë“  ì¤‘ë³µ ê²€ì‚¬ë¥¼ í†µê³¼í•˜ë©´ ë°ì´í„° ì¶”ê°€
    all_data.append({"ì œëª©": title, "ì‘ì„±ì¼": date, "ë³¸ë¬¸ë‚´ìš©": content, "ë§í¬": link, "ì‚¬ì§„": ";".join(images)})
    return True

# --- í¬ë¡¤ë§ í•¨ìˆ˜ ---
def extract_notice_board_urls(college_pages_list):
    department_boards_dict = {}
    base_wwwk_url = "https://wwwk.kangwon.ac.kr"
    print("ğŸ“œ ë‹¨ê³¼ëŒ€í•™ í˜ì´ì§€ì—ì„œ í•™ê³¼ë³„ ê³µì§€ì‚¬í•­ ê²Œì‹œíŒ URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
    for college in tqdm(college_pages_list, desc="ë‹¨ê³¼ëŒ€í•™ë³„ URL ìˆ˜ì§‘ ì¤‘"):
        soup = get_soup(college['url'])
        if not soup: continue
        blocks = soup.select("div.box.temp_titbox")
        for block in blocks:
            dept = block.select_one("h4.h0")
            link = block.select_one("ul.shortcut li:last-child a")
            if dept and link:
                name = dept.text.strip().split('\n')[0]
                if name in manual_board_mapping: continue
                href = link.get("href")
                if href:
                    url = urljoin(base_wwwk_url, href)
                    url = url.replace("wwwk.kangwon.ac.kr/wwwk.kangwon.ac.kr", "wwwk.kangwon.ac.kr")
                    department_boards_dict[name] = url
        time.sleep(0.1)
        
    for dept_name, manual_url in manual_board_mapping.items():
        department_boards_dict[dept_name] = manual_url
    print(f"âœ… ì´ {len(department_boards_dict)}ê°œì˜ í•™ê³¼ ê³µì§€ì‚¬í•­ URL ìˆ˜ì§‘ ì™„ë£Œ.")
    return department_boards_dict

def crawl_all_departments(board_dict, start_date_obj, max_page=None):
    def append_offset(url, offset):
        parsed = urlparse(url)
        query = parse_qs(parsed.query)
        query['article.offset'] = [str(offset)]
        new_query = urlencode(query, doseq=True)
        return urlunparse(parsed._replace(query=new_query))

    for dept, url in tqdm(board_dict.items(), desc="í•™ê³¼ë³„ ì§„í–‰"):
        print(f"\nğŸ“˜ [{dept}] ì‹œì‘: {url}")
        page = 0
        stop_crawling_this_dept = False
        previous_page_links = set()

        while not stop_crawling_this_dept and (max_page is None or page < max_page):
            page_url = append_offset(url, page * 10)
            soup = get_soup(page_url)
            if not soup: break
            rows = soup.select("tbody tr")

            first_row_text = rows[0].get_text() if rows else ""
            if not rows or (len(rows) == 1 and "ë“±ë¡ëœ ê¸€ì´ ì—†ìŠµë‹ˆë‹¤" in first_row_text):
                if page == 0: print(f"     - ë“±ë¡ëœ ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
                break

            current_page_links = set()
            for row in rows:
                try:
                    is_notice_row = row.select_one("td") and "ê³µì§€" in row.select_one("td").text
                    a_tag = row.select_one("a")
                    if not a_tag: continue
                    
                    raw_href = a_tag.get("href")
                    parsed_href = urlparse(raw_href)
                    query_params = parse_qs(parsed_href.query)
                    query_params.pop('article.offset', None); query_params.pop('pageIndex', None); query_params.pop('page', None)
                    normalized_query = urlencode(query_params, doseq=True)
                    normalized_href = urlunparse(parsed_href._replace(query=normalized_query))
                    current_page_links.add(normalized_href)

                    title = a_tag.get_text(strip=True)
                    href = urljoin(url, raw_href)
                    
                    detail_soup = get_soup(href)
                    if not detail_soup: continue
                    delay_request(0.1, 0.3)
                    
                    date = extract_written_date(detail_soup)

                    if is_too_old(date, start_date_obj):
                        if is_notice_row:
                            print(f"         ğŸŸ  [ì˜¤ë˜ëœ ê³µì§€] {title[:35]} (ë‚ ì§œ: {date}) - ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue
                        else:
                            print(f"   ğŸ”š [{dept}] ì§€ì •ëœ ì‹œì‘ ë‚ ì§œ ì´ì „ ê²Œì‹œë¬¼ ë°œê²¬, ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                            stop_crawling_this_dept = True
                            break
                    
                    content_div = detail_soup.select_one("div.b-content-box div.fr-view") or detail_soup.select_one("div.b-content-box")
                    content = clean_html_keep_table(str(content_div)) if content_div else "(ë³¸ë¬¸ ì—†ìŒ)"
                    
                    img_files = []
                    if content_div:
                        
                        prefix = f"{sanitize_filename(dept)}_{generate_notice_key(title, date)}"
                        folder_path = os.path.join(SAVE_FOLDER, "college_depts", sanitize_filename(dept))
                        
                        for i, img in enumerate(content_div.select("img")):
                            src = img.get("src")
                            if src and not src.startswith("data:"):
                                full_img_url = urljoin(href, src)
                                saved_path = save_image(full_img_url, folder_path, prefix, i, src)
                                if saved_path: img_files.append(saved_path)
                    
                    if add_notice_if_not_duplicate(title, date, content, href, img_files):
                        print(f"         ğŸ“„ [ìˆ˜ì§‘] {title[:40]}")

                except Exception as e:
                    print(f"         âŒ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {title[:30]} ({e})")
            
            if current_page_links == previous_page_links:
                print(f"     - [{dept}] ì´ì „ í˜ì´ì§€ì™€ ë‚´ìš©ì´ ë™ì¼í•˜ì—¬ ì¤‘ë‹¨í•©ë‹ˆë‹¤ (í˜ì´ì§€ë„¤ì´ì…˜ ì—†ìŒ).")
                break
            
            previous_page_links = current_page_links
            if stop_crawling_this_dept: break
            page += 1
            delay_request()
        
        if not stop_crawling_this_dept: print(f"âœ… [{dept}] í™•ì¸ ì™„ë£Œ (ìµœëŒ€ {max_page}í˜ì´ì§€).")

def crawl_mainpage(start_date_obj):
    print("\nğŸ“‚ [ë©”ì¸í˜ì´ì§€] ì‹œì‘")
    BASE_URL = "https://www.kangwon.ac.kr"
    PATH_PREFIX = "/www"
    categories = [
        {"name": "ê³µì§€ì‚¬í•­", "bbsNo": "81", "key": "277"},
        {"name": "í–‰ì‚¬ì•ˆë‚´", "bbsNo": "38", "key": "279"},
        {"name": "ê³µëª¨ëª¨ì§‘", "bbsNo": "345", "key": "1959"},
        {"name": "ì¥í•™ê²Œì‹œíŒ", "bbsNo": "34", "key": "232"},
    ]

    for cat in categories:
        print(f"   â¡ï¸  [{cat['name']}] ìˆ˜ì§‘ ì¤‘...")
        stop_crawling_this_category = False
        for page in range(1, 999): 
            if stop_crawling_this_category:
                break
            
            list_url = f"{BASE_URL}{PATH_PREFIX}/selectBbsNttList.do?bbsNo={cat['bbsNo']}&pageUnit=10&key={cat['key']}&pageIndex={page}"
            soup = get_soup(list_url)
            if not soup:
                break
            
            rows = soup.select("tbody tr")
            if not rows:
                break
            
            for row in rows:
                try:
                    is_notice_row = row.select_one("td") and 'ê³µì§€' in row.select_one("td").get_text(strip=True)
                    a_tag = row.select_one("td.subject a")
                    if not a_tag:
                        continue

                    title = a_tag.get_text(strip=True)
                    href = a_tag.get("href", "")
                    
                    detail_url = ""
                    if "fnSelectBbsNttView" in href:
                        match = re.search(r"fnSelectBbsNttView\('(\d+)',\s*'(\d+)',\s*'(\d+)'\)", href)
                        if not match:
                            continue
                        bbs_no, ntt_no, key_param = match.groups()
                        detail_url = f"{BASE_URL}{PATH_PREFIX}/selectBbsNttView.do?bbsNo={bbs_no}&nttNo={ntt_no}&key={key_param}"
                    else:
                        detail_url = urljoin(f"{BASE_URL}{PATH_PREFIX}/", href)

                    detail_soup = get_soup(detail_url)
                    if not detail_soup:
                        continue
                    delay_request()
                    
                    date = extract_written_date(detail_soup)

                    if is_too_old(date, start_date_obj):
                        if is_notice_row:
                            print(f"         ğŸŸ  [ì˜¤ë˜ëœ ê³µì§€] {title[:35]} - ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue
                        else:
                            print(f"       ğŸ”š [{cat['name']}] ì§€ì •ëœ ì‹œì‘ ë‚ ì§œ ì´ì „ ê²Œì‹œë¬¼ ë°œê²¬, ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                            stop_crawling_this_category = True
                            break
                    
                    # â–¼â–¼â–¼â–¼â–¼ ì£¼ìš” ìˆ˜ì • ë¶€ë¶„ â–¼â–¼â–¼â–¼â–¼
                    content_div = detail_soup.select_one("div#bbs_ntt_cn_con, td.bbs_content")
                    # .get_text() ëŒ€ì‹  clean_html_keep_table í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½
                    content = clean_html_keep_table(str(content_div)) if content_div else "(ë³¸ë¬¸ ì—†ìŒ)"
                    # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
                    
                    img_tags = (content_div.select("img") if content_div else []) + (detail_soup.select("div.photo_area img") if detail_soup.select_one("div.photo_area") else [])
                    image_urls_with_duplicates = [urljoin(detail_url, img.get("src")) for img in img_tags if img.get("src") and not img.get("src").startswith("data:")]
                    images_urls = list(dict.fromkeys(image_urls_with_duplicates))

                    prefix = f"main_{generate_notice_key(title, date)}"
                    save_folder_path = os.path.join(SAVE_FOLDER, "main")
                    img_files = [save_image(link, save_folder_path, prefix, i, link) for i, link in enumerate(images_urls)]
                    img_files = list(filter(None, img_files))

                    if add_notice_if_not_duplicate(title, date, content, detail_url, img_files):
                        print(f"         ğŸ“„ [ìˆ˜ì§‘] {title[:45]}")

                except Exception as e:
                    print(f"       âŒ ë©”ì¸í˜ì´ì§€ ìƒì„¸ ì‹¤íŒ¨: {title[:30]} ({e})")
            
            if stop_crawling_this_category:
                break
            
    print("âœ… [ë©”ì¸í˜ì´ì§€] ì™„ë£Œ")

def crawl_library(start_date_obj):
    print("\nğŸ“‚ [ë„ì„œê´€] ì‹œì‘")
    base_url = "https://library.kangwon.ac.kr"
    list_api = f"{base_url}/pyxis-api/1/bulletin-boards/24/bulletins"
    detail_api_template = f"{base_url}/pyxis-api/1/bulletins/24/{{id}}"
    stop_crawling = False
    
    for page in tqdm(range(0, 999), desc="   [ë„ì„œê´€]", leave=False): 
        if stop_crawling:
            break
        
        params = {"offset": page * 10, "max": 10, "bulletinCategoryId": 1}
        try:
            res = session.get(list_api, headers=HEADERS, params=params, timeout=10)
            res.raise_for_status()
            list_data = res.json().get("data", {}).get("list", [])
            if not list_data:
                break
            
            for item in list_data:
                try:
                    is_notice_item = item.get('isNotice', False)
                    title = item.get('title', '(ì œëª© ì—†ìŒ)')
                    item_id = item.get('id')
                    if not item_id:
                        continue

                    detail_res = session.get(detail_api_template.format(id=item_id), headers=HEADERS, timeout=10)
                    detail_res.raise_for_status()
                    detail_data = detail_res.json().get("data", {})
                    
                    date = detail_data.get("dateCreated", "")[:10]

                    if is_too_old(date, start_date_obj, date_format="%Y-%m-%d"):
                        if is_notice_item:
                            print(f"         ğŸŸ  [ì˜¤ë˜ëœ ê³µì§€] {title[:35]} - ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue
                        else:
                            print(f"       ğŸ”š [ë„ì„œê´€] ì§€ì •ëœ ì‹œì‘ ë‚ ì§œ ì´ì „ ê²Œì‹œë¬¼ ë°œê²¬, ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                            stop_crawling = True
                            break

                    date_for_csv = date.replace("-", ".")
                    detail_url = f"{base_url}/community/bulletin/notice/{item_id}"
                    
                    html_content = detail_data.get("content", "")
                    content_soup = BeautifulSoup(html_content, "html.parser")
                    content = content_soup.get_text("\n", strip=True)
                    
                    img_files = []
                    prefix = f"library_{generate_notice_key(title, date_for_csv)}"
                    for i, img in enumerate(content_soup.select("img")):
                        src = img.get("src")
                        if src and "/pyxis-api/attachments/" in src:
                            full_img_url = urljoin(base_url, src)
                            saved_path = save_image(full_img_url, os.path.join(SAVE_FOLDER, "library"), prefix, i)
                            if saved_path:
                                img_files.append(saved_path)
                    
                    if add_notice_if_not_duplicate(title, date_for_csv, content, detail_url, img_files):
                        print(f"         ğŸ“„ [ìˆ˜ì§‘] {title[:45]}")

                except Exception as e:
                    print(f"     - ë„ì„œê´€ ìƒì„¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            if stop_crawling:
                break
        except requests.exceptions.RequestException as e:
            print(f"     - ë„ì„œê´€ ëª©ë¡ ìš”ì²­ ì‹¤íŒ¨ (page={page}): {e}")
            break
            
    print("âœ… [ë„ì„œê´€] ì™„ë£Œ")

def crawl_engineering(start_date_obj):
    print("\nğŸ“‚ [ê³µí•™êµìœ¡í˜ì‹ ì„¼í„°] ì‹œì‘")
    base_url = "https://icee.kangwon.ac.kr"
    stop_crawling = False
    
    for page in tqdm(range(1, 999), desc="   [ê³µí•™êµìœ¡í˜ì‹ ì„¼í„°]", leave=False):
        if stop_crawling:
            break
        
        list_url = f"{base_url}/index.php?mt=page&mp=5_1&mm=oxbbs&oxid=1&cpage={page}"
        soup = get_soup(list_url)
        if not soup:
            break
            
        rows = soup.select("table.bbs_list tbody tr")
        if not rows:
            break

        for row in rows:
            try:
                a_tag = row.select_one("td.tit a")
                date_td = row.select_one("td.dt")
                if not a_tag or not date_td:
                    continue

                title = a_tag.get_text(strip=True)
                date = date_td.text.strip().replace("-", ".")
                is_notice_row = title.startswith('[ê³µì§€]')

                if is_too_old(date, start_date_obj):
                    if is_notice_row:
                        print(f"         ğŸŸ  [ì˜¤ë˜ëœ ê³µì§€] {title[:35]} - ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue
                    else:
                        print(f"       ğŸ”š [ê³µí•™êµìœ¡í˜ì‹ ì„¼í„°] ì§€ì •ëœ ì‹œì‘ ë‚ ì§œ ì´ì „ ê²Œì‹œë¬¼ ë°œê²¬, ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        stop_crawling = True
                        break

                detail_url = urljoin(base_url, a_tag['href'])
                detail_soup = get_soup(detail_url)
                if not detail_soup:
                    continue
                delay_request()
                
                content_div = detail_soup.select_one("div.view_cont, div.note")
                content = clean_html_keep_table(str(content_div))
                
                img_files = []
                if content_div:
                    prefix = f"engineering_{generate_notice_key(title, date)}"
                    for i, img in enumerate(content_div.select("img")):
                        src = img.get("src")
                        if src and not src.startswith("data:"):
                            full_img_url = urljoin(detail_url, src)
                            saved_path = save_image(full_img_url, os.path.join(SAVE_FOLDER, "engineering"), prefix, i, src)
                            if saved_path:
                                img_files.append(saved_path)
                
                if add_notice_if_not_duplicate(title, date, content, detail_url, img_files):
                    print(f"         ğŸ“„ [ìˆ˜ì§‘] {title[:45]}")

            except Exception as e:
                print(f"     - ê³µí•™êµìœ¡í˜ì‹ ì„¼í„° ìƒì„¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        if stop_crawling:
            break
            
    print("âœ… [ê³µí•™êµìœ¡í˜ì‹ ì„¼í„°] ì™„ë£Œ")

def crawl_international(start_date_obj):
    print("\nğŸ“‚ [êµ­ì œêµë¥˜ì²˜] ì‹œì‘")
    base_url = "https://oiaknu.kangwon.ac.kr"
    path = "/oiaknu/notice.do"
    stop_crawling = False
    
    for offset in tqdm(range(0, 9999, 10), desc="   [êµ­ì œêµë¥˜ì²˜]", leave=False):
        if stop_crawling:
            break
            
        list_url = f"{base_url}{path}?article.offset={offset}"
        soup = get_soup(list_url)
        if not soup:
            break
        
        rows = soup.select("tbody > tr")
        if not rows:
            break

        for row in rows:
            try:
                title_tag = row.select_one("td.b-td-left a")
                date_tag = row.select_one("td:nth-last-child(3)")
                if not title_tag or not date_tag:
                    continue

                title = title_tag.get_text(strip=True)
                date_str = date_tag.get_text(strip=True)
                date = "20" + date_str if date_str and not date_str.startswith("20") else date_str
                is_notice_row = (row.select_one("td").get_text(strip=True) == "ê³µì§€")

                if is_too_old(date, start_date_obj):
                    if is_notice_row:
                        print(f"         ğŸŸ  [ì˜¤ë˜ëœ ê³µì§€] {title[:35]} - ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue
                    else:
                        print(f"       ğŸ”š [êµ­ì œêµë¥˜ì²˜] ì§€ì •ëœ ì‹œì‘ ë‚ ì§œ ì´ì „ ê²Œì‹œë¬¼ ë°œê²¬, ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        stop_crawling = True
                        break

                href = title_tag.get('href', '')
                parsed_href = urlparse(href)
                query_params = parse_qs(parsed_href.query)
                article_no = query_params.get('articleNo', [None])[0]
                if not article_no:
                    continue
                
                detail_url = f"{base_url}{path}?mode=view&articleNo={article_no}"
                detail_soup = get_soup(detail_url)
                if not detail_soup:
                    continue
                delay_request()
                
                content_div = detail_soup.select_one("div.b-content-box")
                content = clean_html_keep_table(str(content_div))
                
                img_files = []
                if content_div:
                    prefix = f"international_{generate_notice_key(title, date)}"
                    for i, img in enumerate(content_div.select("img")):
                        src = img.get("src")
                        if src and not src.startswith("data:"):
                            full_img_url = urljoin(detail_url, src)
                            saved_path = save_image(full_img_url, os.path.join(SAVE_FOLDER, "international"), prefix, i, src)
                            if saved_path:
                                img_files.append(saved_path)
                
                if add_notice_if_not_duplicate(title, date, content, detail_url, img_files):
                    print(f"         ğŸ“„ [ìˆ˜ì§‘] {title[:45]}")

            except Exception as e:
                print(f"     - êµ­ì œêµë¥˜ì²˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        if stop_crawling:
            break
            
    print("âœ… [êµ­ì œêµë¥˜ì²˜] ì™„ë£Œ")


def crawl_sw(start_date_obj):
    print("\nğŸ“‚ [SWì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨] ì‹œì‘")
    base_url = "https://sw.kangwon.ac.kr"
    stop_crawling = False
    
    for page in tqdm(range(1, 100), desc="   [SWì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨]", leave=False):
        if stop_crawling:
            break
            
        list_url = f"{base_url}/index.php?mt=page&mp=5_1&mm=oxbbs&oxid=1&cpage={page}"
        soup = get_soup(list_url)
        if not soup:
            break
        
        rows = soup.select("table.bbs_list > tbody > tr")
        if not rows:
            break

        for row in rows:
            try:
                date_td = row.select_one("td:nth-last-child(2)")
                title_tag = row.select_one("td.tit a")

                if not date_td or not title_tag:
                    continue

                date = date_td.get_text(strip=True).replace("-", ".")
                
                is_notice_row = bool(row.select_one("img[alt='ê³µì§€ê¸€']"))

                if is_too_old(date, start_date_obj):
                    if is_notice_row:
                        print(f"           ğŸŸ  [ì˜¤ë˜ëœ ê³µì§€] ... (ë‚ ì§œ: {date}) - ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue
                    else:
                        print(f"         ğŸ”š [SWì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨] ì§€ì •ëœ ì‹œì‘ ë‚ ì§œ ì´ì „ ê²Œì‹œë¬¼ ë°œê²¬, ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        stop_crawling = True
                        break

                title = title_tag.get_text(strip=True)
                detail_url = urljoin(base_url, title_tag.get("href"))
                
                detail_soup = get_soup(detail_url)
                if not detail_soup:
                    continue
                delay_request()

                content_div = detail_soup.select_one("table.bbs_view td.bbs_td[colspan='6']")
                content = clean_html_keep_table(str(content_div)) if content_div else "(ë³¸ë¬¸ ì—†ìŒ)"

                img_files = []
                if content_div:
                    prefix = f"sw_{generate_notice_key(title, date)}"
                    folder_path = os.path.join(SAVE_FOLDER, "sw")
                    
                    for i, img in enumerate(content_div.select("img")):
                        src = img.get("src")
                        if src and not src.startswith("data:"):
                            full_img_url = urljoin(detail_url, src)
                            saved_path = save_image(full_img_url, folder_path, prefix, i, src)
                            if saved_path:
                                img_files.append(saved_path)

                if add_notice_if_not_duplicate(title, date, content, detail_url, img_files):
                    print(f"           ğŸ“„ [ìˆ˜ì§‘] {title[:45]}")

            except Exception as e:
                print(f"         âŒ SWì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {title[:30]} ({e})")
        
        if stop_crawling:
            break
            
    print("âœ… [SWì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨] ì™„ë£Œ")
    
# --- ë°ì´í„° ì†ŒìŠ¤ ---
college_intro_pages = [
    {'college_name': 'ê°„í˜¸ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1782&'},
    {'college_name': 'ê²½ì˜ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1752&'},
    {'college_name': 'ë†ì—…ìƒëª…ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1758&'},
    {'college_name': 'ë™ë¬¼ìƒëª…ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1761&'},
    {'college_name': 'ë¬¸í™”ì˜ˆìˆ  ê³µê³¼ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1912&'},
    {'college_name': 'ì‚¬ë²”ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1767&'},
    {'college_name': 'ì‚¬íšŒê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1770&'},
    {'college_name': 'ì‚°ë¦¼í™˜ê²½ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1773&'},
    {'college_name': 'ìˆ˜ì˜ê³¼ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1776&'},
    {'college_name': 'ì•½í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1779&'},
    {'college_name': 'ì˜ê³¼ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1975&'},
    {'college_name': 'ì˜ìƒëª…ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1785&'},
    {'college_name': 'ì¸ë¬¸ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1788&'},
    {'college_name': 'ìì—°ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1791&'},
    {'college_name': 'ITëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1794&'},
]
manual_board_mapping = {
    "AIìœµí•©í•™ê³¼": "https://ai.kangwon.ac.kr/ai/community/notice.do",
    "ë””ì§€í„¸ë°€ë¦¬í„°ë¦¬í•™ê³¼": "https://military.kangwon.ac.kr/military/professor/notice.do",
    "ììœ ì „ê³µí•™ë¶€": "https://liberal.kangwon.ac.kr/liberal/community/notice.do",
    "ê¸€ë¡œë²Œìœµí•©í•™ë¶€": "https://globalconvergence.kangwon.ac.kr/globalconvergence/info/undergraduate-community.do",
    "ë¯¸ë˜ìœµí•©ê°€ìƒí•™ê³¼": "https://multimajor.kangwon.ac.kr/multimajor/community/notice.do",
    "ë™ë¬¼ì‚°ì—…ìœµí•©í•™ê³¼": "https://animal.kangwon.ac.kr/animal/community/notice.do"
}

# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
if __name__ == "__main__":
    print("\nğŸš€ ê°•ì›ëŒ€ ì „ì²´ ê³µì§€ í¬ë¡¤ë§ ì‹œì‘")
    
    try:
        START_DATE_OBJ = datetime.strptime(CRAWL_START_DATE, "%Y-%m-%d")
        print(f"ğŸ—“ï¸ ìˆ˜ì§‘ ì‹œì‘ ë‚ ì§œ: {CRAWL_START_DATE} ì´í›„ì˜ ëª¨ë“  ìƒˆ ê²Œì‹œë¬¼ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    except ValueError:
        print(f"âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. 'YYYY-MM-DD' í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”. (ì…ë ¥ê°’: {CRAWL_START_DATE})")
        exit()
    
    load_existing_data()
    file_exists_before_crawl = os.path.exists(CSV_FILE)

    crawl_mainpage(START_DATE_OBJ)
    crawl_library(START_DATE_OBJ)
    crawl_engineering(START_DATE_OBJ)
    crawl_international(START_DATE_OBJ)
    crawl_sw(START_DATE_OBJ)

    boards = extract_notice_board_urls(college_intro_pages)
    if boards:
        crawl_all_departments(boards, START_DATE_OBJ, max_page=None) 

    # ìˆ˜ì§‘ëœ ìƒˆ ë°ì´í„°ë¥¼ íŒŒì¼ì— ì¶”ê°€
    if not all_data:
        print("\nâœ… ì¶”ê°€í•  ìƒˆë¡œìš´ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        try:
            with open(CSV_FILE, "a", newline="", encoding="utf-8-sig") as f:
                writer = csv.DictWriter(f, fieldnames=["ì œëª©", "ì‘ì„±ì¼", "ë³¸ë¬¸ë‚´ìš©", "ë§í¬", "ì‚¬ì§„"])
                if not file_exists_before_crawl or os.path.getsize(CSV_FILE) == 0:
                    writer.writeheader()
                writer.writerows(all_data)
            print(f"\nâœ… ìƒˆë¡œìš´ ê²Œì‹œê¸€ {len(all_data)}ê±´ ì¶”ê°€ ì™„ë£Œ: {CSV_FILE}")
        except Exception as e:
            print(f"âŒ CSV íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")