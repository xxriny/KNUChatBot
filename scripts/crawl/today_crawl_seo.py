import os
import csv
import time
import re
import random
import unicodedata
import io
from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse
import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from tqdm import tqdm
from difflib import SequenceMatcher
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from datetime import datetime, timedelta
from azure.storage.blob import BlobServiceClient
from azure.core.exceptions import AzureError, ResourceNotFoundError  
from dotenv import load_dotenv

# --- .env íŒŒì¼ ë¡œë“œ ---
load_dotenv(override=True)   # â† ì´ê±¸ë¡œ ê¸°ì¡´ í™˜ê²½ë³€ìˆ˜ ìœ„ì— ë®ì–´ì“°ê¸°

# --- ê¸°ë³¸ ì„¤ì • ---
HEADERS = {"User-Agent": "Mozilla/5.0"}
# SAVE_FOLDERëŠ” ì´ì œ ë¡œì»¬ ê²½ë¡œê°€ ì•„ë‹Œ, Blob ê²½ë¡œì˜ ê¸°ë°˜ìœ¼ë¡œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.
SAVE_FOLDER = "images"
CSV_FILE = "kangwon_notices.csv"   # Blobì— ì €ì¥í•  ë¸”ë¡­ ì´ë¦„
# âœ… ë¡œì»¬ CSV ì €ì¥ ê²½ë¡œ(ê³ ì •)
CSV_PATH = "/home/data/extracted-app/data/kangwon_notices.csv"

# â–¼â–¼â–¼ í¬ë¡¤ë§ ì‹œì‘ ë‚ ì§œ ì„¤ì • â–¼â–¼â–¼
CRAWL_START_DATE = (datetime.now() - timedelta(days=3)).strftime("%Y-%m-%d")
# â–²â–²â–² ì„¤ì • ì™„ë£Œ â–²â–²â–²

# â–¼â–¼â–¼ Azure Blob Storage ì„¤ì • (í•„ìˆ˜) â–¼â–¼â–¼
AZURE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING")
AZURE_CONTAINER_NAME = "images"            # ì´ë¯¸ì§€ ì €ì¥ ì»¨í…Œì´ë„ˆ
AZURE_CSV_CONTAINER_NAME = "data"          # CSV ì €ì¥ ì»¨í…Œì´ë„ˆ

# Azure ì„œë¹„ìŠ¤ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
blob_service_client = None
try:
    if AZURE_CONNECTION_STRING:
        blob_service_client = BlobServiceClient.from_connection_string(AZURE_CONNECTION_STRING)
        print("â˜ï¸  Azure Blob Storage í´ë¼ì´ì–¸íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("â€¼ï¸  ì¤‘ìš”: AZURE_STORAGE_CONNECTION_STRING í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€/CSVì˜ Blob ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
except Exception as e:
    print(f"âŒ Azure í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. ì´ë¯¸ì§€/CSVì˜ Blob ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

# --- ì„¸ì…˜ ë° ì¬ì‹œë„ ì„¤ì • ---
session = requests.Session()
retry = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
adapter = HTTPAdapter(max_retries=retry)
session.mount("http://", adapter)
session.mount("https://", adapter)

# --- ì „ì—­ ë³€ìˆ˜ ---
all_data = []
pre_existing_data = []  # ê¸°ì¡´ CSV ë°ì´í„°ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
existing_keys_set = set()

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---
def sanitize_filename(name):
    return re.sub(r'[^\w\s_.-]', '_', name).strip()

def save_image(img_url, folder, prefix, idx, original_name="image.jpg", referer_url=None):
    """
    ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ Azure Blob Storageì— ì €ì¥í•©ë‹ˆë‹¤.
    Azure í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì´ë¯¸ì§€ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.
    """
    if not blob_service_client:
        return None

    try:
        img_headers = HEADERS.copy()
        if referer_url:
            img_headers['Referer'] = referer_url

        res = session.get(img_url, headers=img_headers, timeout=10)
        time.sleep(random.uniform(0.1, 0.3))

        if not (res.status_code == 200 and len(res.content) > 1024):
            return None

        ext = os.path.splitext(original_name)[1] or '.jpg'
        if len(ext) > 5:
            ext = '.jpg'
        filename = sanitize_filename(f"{prefix}_{idx}{ext}")

        blob_path_prefix = os.path.relpath(folder, SAVE_FOLDER).replace("\\", "/")
        blob_name = f"{blob_path_prefix}/{filename}" if blob_path_prefix != '.' else filename

        blob_client = blob_service_client.get_blob_client(container=AZURE_CONTAINER_NAME, blob=blob_name)
        blob_client.upload_blob(res.content, overwrite=True)

        return blob_client.url

    except AzureError as e:
        print(f"      âŒ Azure ì—…ë¡œë“œ ì‹¤íŒ¨: {img_url} ({e})")
    except Exception as e:
        print(f"      âš ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬/ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {img_url} ({e})")

    return None

def clean_html_keep_table(raw_html):
    soup = BeautifulSoup(raw_html, 'html.parser')
    for zoom_element in soup.select('span.photo_zoom'):
        zoom_element.decompose()
    output = []
    for table in soup.find_all('table'):
        output.append(extract_table_text(table))
        table.decompose()
    for br in soup.find_all("br"):
        br.replace_with("\n")
    for elem in soup.find_all(['p', 'div', 'span']):
        text = elem.get_text(strip=True, separator="\n")
        if text and text not in output:
            output.append(text)
    return "\n".join(output).strip()

def extract_table_text(table):
    rows = table.find_all('tr')
    return '\n'.join(
        ' | '.join(col.get_text(strip=True) for col in row.find_all(['td', 'th']) if col.get_text(strip=True))
        for row in rows if row.find_all(['td', 'th'])
    )

def extract_written_date(soup):
    selectors = [
        "table.bbs_view tr:nth-of-type(2) td:nth-of-type(2)",  # SWì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨ í˜•ì‹
        "dd.info div.date",                                    # ê³µí•™êµìœ¡í˜ì‹ ì„¼í„° í˜•ì‹
        "div.bbs_right span:last-child",                       # ë©”ì¸í˜ì´ì§€ ê³µì§€ì‚¬í•­ í˜•ì‹
        "li.b-date-box span:last-child",                       # í•™ê³¼ í˜ì´ì§€ í˜•ì‹
        "div.b-etc-box li.b-date-box span",
        "dl.date dd",
        "span.date"
    ]

    for selector in selectors:
        date_tag = soup.select_one(selector)
        if date_tag:
            date_text = date_tag.get_text(strip=True)
            match = re.search(r'(20\d{2}[.\së…„-]+[01]?\d[.\sì›”-]+[0-3]?\d+)', date_text)
            if match:
                cleaned_date = match.group(1)
                cleaned_date = cleaned_date.replace('ë…„', '.').replace('ì›”', '.').replace('ì¼', '').replace('-', '.').replace(' ', '')
                return cleaned_date.strip('.')

    full_text = soup.get_text(" ", strip=True)
    match = re.search(r'(20\d{2}[.\-/ë…„\s]+[01]?\d[.\-/ì›”\s]+[0-3]?\d+)', full_text)
    if match:
        cleaned_date = match.group(1)
        cleaned_date = cleaned_date.replace('ë…„', '.').replace('ì›”', '.').replace('ì¼', '').replace(' ', '').replace('-', '.')
        return cleaned_date.strip('.')

    return "(ì‘ì„±ì¼ ì—†ìŒ)"

def generate_notice_key(title, date):
    temp_title = title if title else ""
    date_str = date if date else ""
    processed_title = re.sub(r'\[[^\]]+\]|\([^\)]+\)|<[^>]+>|ã€[^ã€‘]+ã€‘', '', temp_title)
    processed_title = unicodedata.normalize('NFKC', processed_title)
    processed_title = re.sub(r'[^a-zA-Z0-9ê°€-í£]', '', processed_title).lower()
    date_str = re.sub(r'[^0-9]', '', date_str)
    return f"{processed_title}_{date_str}"

def get_soup(url):
    try:
        response = session.get(url, headers=HEADERS, timeout=15)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        return BeautifulSoup(response.text, 'html.parser')
    except Exception as e:
        print(f"      âŒ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {url} ({e})")
        return None

def delay_request(min_sec=0.5, max_sec=1.5):
    time.sleep(random.uniform(min_sec, max_sec))

def is_too_old(post_date_str, start_date_obj, date_format="%Y.%m.%d"):
    """ê²Œì‹œë¬¼ ë‚ ì§œê°€ ì§€ì •ëœ ì‹œì‘ ë‚ ì§œë³´ë‹¤ ì˜¤ë˜ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
    if not post_date_str:
        return False
    try:
        post_date_obj = datetime.strptime(post_date_str, date_format)
        return post_date_obj < start_date_obj
    except (ValueError, TypeError):
        return False

# --- ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ---
def load_existing_data():
    """
    1ìˆœìœ„: Azure Blob Storageì—ì„œ CSV ë‹¤ìš´ë¡œë“œ â†’ pre_existing_data ë¡œë“œ
    2ìˆœìœ„: Blobì´ ì—†ê±°ë‚˜ ì‹¤íŒ¨ ì‹œ ë¡œì»¬ CSV_PATHì—ì„œ ë¡œë“œ (fallback)
    """
    global pre_existing_data, existing_keys_set

    # 1) Azure Blob ì‹œë„
    if blob_service_client:
        try:
            blob_client = blob_service_client.get_blob_client(container=AZURE_CSV_CONTAINER_NAME, blob=CSV_FILE)
            print(f"ğŸ“„ Azure Storageì—ì„œ '{CSV_FILE}' íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
            downloader = blob_client.download_blob()  # exists() í˜¸ì¶œ ì—†ì´ ë°”ë¡œ ì‹œë„
            blob_bytes = downloader.readall()
            blob_string = blob_bytes.decode("utf-8-sig")

            csv_file_in_memory = io.StringIO(blob_string)
            reader = csv.DictReader(csv_file_in_memory)

            if "ì œëª©" not in reader.fieldnames or "ì‘ì„±ì¼" not in reader.fieldnames:
                print(f"âš ï¸ '{CSV_FILE}'ì— 'ì œëª©' ë˜ëŠ” 'ì‘ì„±ì¼' ì»¬ëŸ¼ì´ ì—†ì–´ ì¤‘ë³µ ê²€ì‚¬ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                pre_existing_data = list(reader)
                existing_keys_set = {
                    generate_notice_key(row.get('ì œëª©', ''), row.get('ì‘ì„±ì¼', ''))
                    for row in pre_existing_data
                }
                print(f"âœ… ê¸°ì¡´ ë°ì´í„° {len(pre_existing_data)}ê±´ ë¡œë“œ ë° ì¤‘ë³µ ê²€ì‚¬ìš© key {len(existing_keys_set)}ê°œ ìƒì„± ì™„ë£Œ.")
                return  # ì„±ê³µí–ˆìœ¼ë¯€ë¡œ í•¨ìˆ˜ ì¢…ë£Œ

        except ResourceNotFoundError:
            print(f"ğŸ“„ Azure Storageì— '{CSV_FILE}' íŒŒì¼ì´ ì—†ì–´ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ Azure Storageì—ì„œ CSV íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # 2) ë¡œì»¬ CSV fallback
    try:
        if os.path.exists(CSV_PATH):
            print(f"ğŸ“„ ë¡œì»¬ì—ì„œ ê¸°ì¡´ CSV ë¡œë“œ: {CSV_PATH}")
            with open(CSV_PATH, "r", encoding="utf-8-sig", newline="") as f:
                reader = csv.DictReader(f)
                if "ì œëª©" in reader.fieldnames and "ì‘ì„±ì¼" in reader.fieldnames:
                    pre_existing_data = list(reader)
                    existing_keys_set = {
                        generate_notice_key(row.get('ì œëª©', ''), row.get('ì‘ì„±ì¼', ''))
                        for row in pre_existing_data
                    }
                    print(f"âœ… (ë¡œì»¬) ê¸°ì¡´ ë°ì´í„° {len(pre_existing_data)}ê±´ ë¡œë“œ ì™„ë£Œ.")
                else:
                    print("âš ï¸ (ë¡œì»¬) CSV ì»¬ëŸ¼ì— 'ì œëª©' ë˜ëŠ” 'ì‘ì„±ì¼'ì´ ì—†ì–´ ì¤‘ë³µê²€ì‚¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        else:
            print(f"â„¹ï¸ ë¡œì»¬ CSVê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {CSV_PATH}")
    except Exception as e:
        print(f"âŒ ë¡œì»¬ CSV ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

def add_notice_if_not_duplicate(title, date, content, link, images):
    """ì¤‘ë³µì„ ê²€ì‚¬í•˜ê³ , ì¤‘ë³µì´ ì•„ë‹ ê²½ìš°ì—ë§Œ ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
    def _is_similar(title1, title2):
        SEQ_MATCHER_THRESHOLD = 0.9
        COSINE_SIMILARITY_THRESHOLD = 0.8

        def normalize_for_similarity(text):
            processed = unicodedata.normalize('NFKC', text.lower())
            processed = re.sub(r'[^\w\sê°€-í£]', '', processed)
            return ' '.join(processed.split())

        norm_title1 = normalize_for_similarity(title1)
        norm_title2 = normalize_for_similarity(title2)

        if not norm_title1 or not norm_title2:
            return False

        seq_ratio = SequenceMatcher(None, norm_title1, norm_title2).ratio()

        try:
            vectorizer = TfidfVectorizer()
            tfidf_matrix = vectorizer.fit_transform([norm_title1, norm_title2])
            cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        except ValueError:
            cosine_sim = 0

        if seq_ratio >= SEQ_MATCHER_THRESHOLD and cosine_sim >= COSINE_SIMILARITY_THRESHOLD:
            print(f"      ğŸš« [ì¤‘ë³µ-ìœ ì‚¬ë„] (Seq: {seq_ratio:.2f}, Cos: {cosine_sim:.2f}) {title[:40]}")
            return True
        return False

    DATE_WINDOW_DAYS = 3
    new_key = generate_notice_key(title, date)

    try:
        new_date_obj = datetime.strptime(date, "%Y.%m.%d")
    except (ValueError, TypeError):
        new_date_obj = None

    # 1. í˜„ì¬ ì„¸ì…˜ ë‚´ ì¤‘ë³µ ê²€ì‚¬
    for post_in_session in all_data:
        if new_key == generate_notice_key(post_in_session['ì œëª©'], post_in_session['ì‘ì„±ì¼']):
            print(f"      ğŸš« [ì¤‘ë³µ-ì„¸ì…˜/ì™„ì „ì¼ì¹˜] {title[:40]}")
            return False

        if new_date_obj:
            try:
                session_date_obj = datetime.strptime(post_in_session['ì‘ì„±ì¼'], "%Y.%m.%d")
                if abs((new_date_obj - session_date_obj).days) <= DATE_WINDOW_DAYS:
                    if _is_similar(title, post_in_session['ì œëª©']):
                        return False
            except (ValueError, TypeError):
                continue

    # 2. ê¸°ì¡´ íŒŒì¼ ë°ì´í„°ì™€ ì¤‘ë³µ ê²€ì‚¬
    if new_date_obj and pre_existing_data:
        start_date_window = new_date_obj - timedelta(days=DATE_WINDOW_DAYS)
        for existing_post in pre_existing_data:
            try:
                existing_date_obj = datetime.strptime(existing_post.get('ì‘ì„±ì¼', ''), "%Y.%m.%d")

                if start_date_window <= existing_date_obj <= new_date_obj:
                    existing_title = existing_post.get('ì œëª©', '')
                    if new_key == generate_notice_key(existing_title, existing_post.get('ì‘ì„±ì¼', '')):
                        print(f"      ğŸš« [ì¤‘ë³µ-ê¸°ì¡´íŒŒì¼/ì™„ì „ì¼ì¹˜] {title[:40]}")
                        return False
                    if _is_similar(title, existing_title):
                        return False
            except (ValueError, TypeError):
                continue

    all_data.append({"ì œëª©": title, "ì‘ì„±ì¼": date, "ë³¸ë¬¸ë‚´ìš©": content, "ë§í¬": link, "ì‚¬ì§„": ";".join(images)})
    return True

# --- í¬ë¡¤ë§ í•¨ìˆ˜ ---
def extract_notice_board_urls(college_pages_list):
    department_boards_dict = {}
    base_wwwk_url = "https://wwwk.kangwon.ac.kr"
    print("ğŸ“œ ë‹¨ê³¼ëŒ€í•™ í˜ì´ì§€ì—ì„œ í•™ê³¼ë³„ ê³µì§€ì‚¬í•­ ê²Œì‹œíŒ URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
    for college in tqdm(college_pages_list, desc="ë‹¨ê³¼ëŒ€í•™ë³„ URL ìˆ˜ì§‘ ì¤‘"):
        soup = get_soup(college['url'])
        if not soup:
            continue
        blocks = soup.select("div.box.temp_titbox")
        for block in blocks:
            dept = block.select_one("h4.h0")
            link = block.select_one("ul.shortcut li:last-child a")
            if dept and link:
                name = dept.text.strip().split('\n')[0]
                if name in manual_board_mapping:
                    continue
                href = link.get("href")
                if href:
                    url = urljoin(base_wwwk_url, href)
                    url = url.replace("wwwk.kangwon.ac.kr/wwwk.kangwon.ac.kr", "wwwk.kangwon.ac.kr")
                    department_boards_dict[name] = url
        time.sleep(0.1)

    for dept_name, manual_url in manual_board_mapping.items():
        department_boards_dict[dept_name] = manual_url
    print(f"âœ… ì´ {len(department_boards_dict)}ê°œì˜ í•™ê³¼ ê³µì§€ì‚¬í•­ URL ìˆ˜ì§‘ ì™„ë£Œ.")
    return department_boards_dict

def crawl_all_departments(board_dict, start_date_obj, max_page=None):
    def append_offset(url, offset):
        parsed = urlparse(url)
        query = parse_qs(parsed.query)
        query['article.offset'] = [str(offset)]
        new_query = urlencode(query, doseq=True)
        return urlunparse(parsed._replace(query=new_query))

    for dept, url in tqdm(board_dict.items(), desc="í•™ê³¼ë³„ ì§„í–‰"):
        print(f"\nğŸ“˜ [{dept}] ì‹œì‘: {url}")
        page = 0
        stop_crawling_this_dept = False
        previous_page_links = set()

        while not stop_crawling_this_dept and (max_page is None or page < max_page):
            page_url = append_offset(url, page * 10)
            soup = get_soup(page_url)
            if not soup:
                break
            rows = soup.select("tbody tr")

            first_row_text = rows[0].get_text() if rows else ""
            if not rows or (len(rows) == 1 and "ë“±ë¡ëœ ê¸€ì´ ì—†ìŠµë‹ˆë‹¤" in first_row_text):
                if page == 0:
                    print(f"      - ë“±ë¡ëœ ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
                break

            current_page_links = set()
            for row in rows:
                try:
                    is_notice_row = row.select_one("td") and "ê³µì§€" in row.select_one("td").text
                    a_tag = row.select_one("a")
                    if not a_tag:
                        continue

                    raw_href = a_tag.get("href")
                    parsed_href = urlparse(raw_href)
                    query_params = parse_qs(parsed_href.query)
                    query_params.pop('article.offset', None); query_params.pop('pageIndex', None); query_params.pop('page', None)
                    normalized_query = urlencode(query_params, doseq=True)
                    normalized_href = urlunparse(parsed_href._replace(query=normalized_query))
                    current_page_links.add(normalized_href)

                    title = a_tag.get_text(strip=True)
                    href = urljoin(url, raw_href)

                    detail_soup = get_soup(href)
                    if not detail_soup:
                        continue
                    delay_request(0.1, 0.3)

                    date = extract_written_date(detail_soup)

                    if is_too_old(date, start_date_obj):
                        if is_notice_row:
                            continue
                        else:
                            print(f"  ğŸ”š [{dept}] ì§€ì •ëœ ì‹œì‘ ë‚ ì§œ ì´ì „ ê²Œì‹œë¬¼ ë°œê²¬, ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                            stop_crawling_this_dept = True
                            break

                    content_div = detail_soup.select_one("div.b-content-box div.fr-view") or detail_soup.select_one("div.b-content-box")
                    content = clean_html_keep_table(str(content_div)) if content_div else "(ë³¸ë¬¸ ì—†ìŒ)"

                    img_files = []
                    if content_div:
                        prefix = f"{sanitize_filename(dept)}_{generate_notice_key(title, date)}"
                        folder_path = os.path.join(SAVE_FOLDER, "college_depts", sanitize_filename(dept))

                        for i, img in enumerate(content_div.select("img")):
                            src = img.get("src")
                            if src and not src.startswith("data:"):
                                full_img_url = urljoin(href, src)
                                saved_path = save_image(full_img_url, folder_path, prefix, i, original_name=src, referer_url=href)
                                if saved_path:
                                    img_files.append(saved_path)

                    if add_notice_if_not_duplicate(title, date, content, href, img_files):
                        print(f"      ğŸ“„ [ìˆ˜ì§‘] {title[:40]}")

                except Exception as e:
                    print(f"      âŒ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {title[:30]} ({e})")

            if current_page_links == previous_page_links:
                print(f"      - [{dept}] ì´ì „ í˜ì´ì§€ì™€ ë‚´ìš©ì´ ë™ì¼í•˜ì—¬ ì¤‘ë‹¨í•©ë‹ˆë‹¤ (í˜ì´ì§€ë„¤ì´ì…˜ ì—†ìŒ).")
                break

            previous_page_links = current_page_links
            if stop_crawling_this_dept:
                break
            page += 1
            delay_request()

        if not stop_crawling_this_dept:
            print(f"âœ… [{dept}] í™•ì¸ ì™„ë£Œ.")

def crawl_mainpage(start_date_obj):
    print("\nğŸ“‚ [ë©”ì¸í˜ì´ì§€] ì‹œì‘")
    BASE_URL = "https://www.kangwon.ac.kr"
    PATH_PREFIX = "/www"
    categories = [
        {"name": "ê³µì§€ì‚¬í•­", "bbsNo": "81", "key": "277"},
        {"name": "í–‰ì‚¬ì•ˆë‚´", "bbsNo": "38", "key": "279"},
        {"name": "ê³µëª¨ëª¨ì§‘", "bbsNo": "345", "key": "1959"},
        {"name": "ì¥í•™ê²Œì‹œíŒ", "bbsNo": "34", "key": "232"},
        {"name": "ì·¨ì—…ì •ë³´", "bbsNo": "117", "key": "768"},
    ]

    for cat in categories:
        print(f"  â¡ï¸  [{cat['name']}] ìˆ˜ì§‘ ì¤‘...")
        stop_crawling_this_category = False
        for page in range(1, 999):
            if stop_crawling_this_category:
                break

            list_url = f"{BASE_URL}{PATH_PREFIX}/selectBbsNttList.do?bbsNo={cat['bbsNo']}&pageUnit=10&key={cat['key']}&pageIndex={page}"
            soup = get_soup(list_url)
            if not soup:
                break

            rows = soup.select("tbody tr")
            if not rows:
                break

            for row in rows:
                try:
                    is_notice_row = row.select_one("td") and 'ê³µì§€' in row.select_one("td").get_text(strip=True)
                    a_tag = row.select_one("td.subject a")
                    if not a_tag:
                        continue

                    title = a_tag.get_text(strip=True)
                    href = a_tag.get("href", "")

                    detail_url = ""
                    if "fnSelectBbsNttView" in href:
                        match = re.search(r"fnSelectBbsNttView\('(\d+)',\s*'(\d+)',\s*'(\d+)'\)", href)
                        if not match:
                            continue
                        bbs_no, ntt_no, key_param = match.groups()
                        detail_url = f"{BASE_URL}{PATH_PREFIX}/selectBbsNttView.do?bbsNo={bbs_no}&nttNo={ntt_no}&key={key_param}"
                    else:
                        detail_url = urljoin(f"{BASE_URL}{PATH_PREFIX}/", href)

                    detail_soup = get_soup(detail_url)
                    if not detail_soup:
                        continue
                    delay_request()

                    date = extract_written_date(detail_soup)

                    if is_too_old(date, start_date_obj):
                        if is_notice_row:
                            continue
                        else:
                            print(f"      ğŸ”š [{cat['name']}] ì§€ì •ëœ ì‹œì‘ ë‚ ì§œ ì´ì „ ê²Œì‹œë¬¼ ë°œê²¬, ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                            stop_crawling_this_category = True
                            break

                    content_div = detail_soup.select_one("div#bbs_ntt_cn_con, td.bbs_content")
                    content = clean_html_keep_table(str(content_div)) if content_div else "(ë³¸ë¬¸ ì—†ìŒ)"

                    img_tags = (content_div.select("img") if content_div else []) + (detail_soup.select("div.photo_area img") if detail_soup.select_one("div.photo_area") else [])
                    image_urls_with_duplicates = [urljoin(detail_url, img.get("src")) for img in img_tags if img.get("src") and not img.get("src").startswith("data:")]
                    images_urls = list(dict.fromkeys(image_urls_with_duplicates))

                    prefix = f"main_{generate_notice_key(title, date)}"
                    save_folder_path = os.path.join(SAVE_FOLDER, "main")
                    img_files = [save_image(link, save_folder_path, prefix, i, link) for i, link in enumerate(images_urls)]
                    img_files = list(filter(None, img_files))

                    if add_notice_if_not_duplicate(title, date, content, detail_url, img_files):
                        print(f"      ğŸ“„ [ìˆ˜ì§‘] {title[:45]}")

                except Exception as e:
                    print(f"      âŒ ë©”ì¸í˜ì´ì§€ ìƒì„¸ ì‹¤íŒ¨: {title[:30]} ({e})")

            if stop_crawling_this_category:
                break

    print("âœ… [ë©”ì¸í˜ì´ì§€] ì™„ë£Œ")

def crawl_library(start_date_obj):
    print("\nğŸ“‚ [ë„ì„œê´€] ì‹œì‘")
    base_url = "https://library.kangwon.ac.kr"
    list_api = f"{base_url}/pyxis-api/1/bulletin-boards/24/bulletins"
    detail_api_template = f"{base_url}/pyxis-api/1/bulletins/24/{{id}}"
    stop_crawling = False

    for page in tqdm(range(0, 999), desc="  [ë„ì„œê´€]", leave=False):
        if stop_crawling:
            break

        params = {"offset": page * 10, "max": 10, "bulletinCategoryId": 1}
        try:
            res = session.get(list_api, headers=HEADERS, params=params, timeout=10)
            res.raise_for_status()
            list_data = res.json().get("data", {}).get("list", [])
            if not list_data:
                break

            for item in list_data:
                try:
                    is_notice_item = item.get('isNotice', False)
                    title = item.get('title', '(ì œëª© ì—†ìŒ)')
                    item_id = item.get('id')
                    if not item_id:
                        continue

                    detail_res = session.get(detail_api_template.format(id=item_id), headers=HEADERS, timeout=10)
                    detail_res.raise_for_status()
                    detail_data = detail_res.json().get("data", {})

                    date = detail_data.get("dateCreated", "")[:10]

                    if is_too_old(date, start_date_obj, date_format="%Y-%m-%d"):
                        if is_notice_item:
                            continue
                        else:
                            print(f"      ğŸ”š [ë„ì„œê´€] ì§€ì •ëœ ì‹œì‘ ë‚ ì§œ ì´ì „ ê²Œì‹œë¬¼ ë°œê²¬, ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                            stop_crawling = True
                            break

                    date_for_csv = date.replace("-", ".")
                    detail_url = f"{base_url}/community/bulletin/notice/{item_id}"

                    html_content = detail_data.get("content", "")
                    content_soup = BeautifulSoup(html_content, "html.parser")
                    content = content_soup.get_text("\n", strip=True)

                    img_files = []
                    prefix = f"library_{generate_notice_key(title, date_for_csv)}"
                    folder_path = os.path.join(SAVE_FOLDER, "library")
                    for i, img in enumerate(content_soup.select("img")):
                        src = img.get("src")
                        if src and "/pyxis-api/attachments/" in src:
                            full_img_url = urljoin(base_url, src)
                            saved_path = save_image(full_img_url, folder_path, prefix, i)
                            if saved_path:
                                img_files.append(saved_path)

                    if add_notice_if_not_duplicate(title, date_for_csv, content, detail_url, img_files):
                        print(f"      ğŸ“„ [ìˆ˜ì§‘] {title[:45]}")

                except Exception as e:
                    print(f"      - ë„ì„œê´€ ìƒì„¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

            if stop_crawling:
                break
        except requests.exceptions.RequestException as e:
            print(f"      - ë„ì„œê´€ ëª©ë¡ ìš”ì²­ ì‹¤íŒ¨ (page={page}): {e}")
            break

    print("âœ… [ë„ì„œê´€] ì™„ë£Œ")

def crawl_engineering(start_date_obj):
    print("\nğŸ“‚ [ê³µí•™êµìœ¡í˜ì‹ ì„¼í„°] ì‹œì‘")
    base_url = "https://icee.kangwon.ac.kr"
    stop_crawling = False

    for page in tqdm(range(1, 999), desc="  [ê³µí•™êµìœ¡í˜ì‹ ì„¼í„°]", leave=False):
        if stop_crawling:
            break

        list_url = f"{base_url}/index.php?mt=page&mp=5_1&mm=oxbbs&oxid=1&cpage={page}"
        soup = get_soup(list_url)
        if not soup:
            break

        rows = soup.select("table.bbs_list tbody tr")
        if not rows:
            break

        for row in rows:
            try:
                a_tag = row.select_one("td.tit a")
                date_td = row.select_one("td.dt")
                if not a_tag or not date_td:
                    continue

                title = a_tag.get_text(strip=True)
                date = date_td.text.strip().replace("-", ".")
                is_notice_row = title.startswith('[ê³µì§€]')

                if is_too_old(date, start_date_obj):
                    if is_notice_row:
                        continue
                    else:
                        print(f"      ğŸ”š [ê³µí•™êµìœ¡í˜ì‹ ì„¼í„°] ì§€ì •ëœ ì‹œì‘ ë‚ ì§œ ì´ì „ ê²Œì‹œë¬¼ ë°œê²¬, ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        stop_crawling = True
                        break

                detail_url = urljoin(base_url, a_tag['href'])
                detail_soup = get_soup(detail_url)
                if not detail_soup:
                    continue
                delay_request()

                content_div = detail_soup.select_one("div.view_cont, div.note")
                content = clean_html_keep_table(str(content_div))

                img_files = []
                if content_div:
                    prefix = f"engineering_{generate_notice_key(title, date)}"
                    folder_path = os.path.join(SAVE_FOLDER, "engineering")
                    for i, img in enumerate(content_div.select("img")):
                        src = img.get("src")
                        if src and not src.startswith("data:"):
                            full_img_url = urljoin(detail_url, src)
                            saved_path = save_image(full_img_url, folder_path, prefix, i, src)
                            if saved_path:
                                img_files.append(saved_path)

                if add_notice_if_not_duplicate(title, date, content, detail_url, img_files):
                    print(f"      ğŸ“„ [ìˆ˜ì§‘] {title[:45]}")

            except Exception as e:
                print(f"      - ê³µí•™êµìœ¡í˜ì‹ ì„¼í„° ìƒì„¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

        if stop_crawling:
            break

    print("âœ… [ê³µí•™êµìœ¡í˜ì‹ ì„¼í„°] ì™„ë£Œ")

def crawl_international(start_date_obj):
    print("\nğŸ“‚ [êµ­ì œêµë¥˜ì²˜] ì‹œì‘")
    base_url = "https://oiaknu.kangwon.ac.kr"
    path = "/oiaknu/notice.do"
    stop_crawling = False

    for offset in tqdm(range(0, 9999, 10), desc="  [êµ­ì œêµë¥˜ì²˜]", leave=False):
        if stop_crawling:
            break

        list_url = f"{base_url}{path}?article.offset={offset}"
        soup = get_soup(list_url)
        if not soup:
            break

        rows = soup.select("tbody > tr")
        if not rows:
            break

        for row in rows:
            try:
                title_tag = row.select_one("td.b-td-left a")
                date_tag = row.select_one("td:nth-last-child(3)")
                if not title_tag or not date_tag:
                    continue

                title = title_tag.get_text(strip=True)
                date_str = date_tag.get_text(strip=True)
                date = "20" + date_str if date_str and not date_str.startswith("20") else date_str
                is_notice_row = (row.select_one("td").get_text(strip=True) == "ê³µì§€")

                if is_too_old(date, start_date_obj):
                    if is_notice_row:
                        continue
                    else:
                        print(f"      ğŸ”š [êµ­ì œêµë¥˜ì²˜] ì§€ì •ëœ ì‹œì‘ ë‚ ì§œ ì´ì „ ê²Œì‹œë¬¼ ë°œê²¬, ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        stop_crawling = True
                        break

                href = title_tag.get('href', '')
                parsed_href = urlparse(href)
                query_params = parse_qs(parsed_href.query)
                article_no = query_params.get('articleNo', [None])[0]
                if not article_no:
                    continue

                detail_url = f"{base_url}{path}?mode=view&articleNo={article_no}"
                detail_soup = get_soup(detail_url)
                if not detail_soup:
                    continue
                delay_request()

                content_div = detail_soup.select_one("div.b-content-box")
                content = clean_html_keep_table(str(content_div))

                img_files = []
                if content_div:
                    prefix = f"international_{generate_notice_key(title, date)}"
                    folder_path = os.path.join(SAVE_FOLDER, "international")
                    for i, img in enumerate(content_div.select("img")):
                        src = img.get("src")
                        if src and not src.startswith("data:"):
                            full_img_url = urljoin(detail_url, src)
                            saved_path = save_image(full_img_url, folder_path, prefix, i, src)
                            if saved_path:
                                img_files.append(saved_path)

                if add_notice_if_not_duplicate(title, date, content, detail_url, img_files):
                    print(f"      ğŸ“„ [ìˆ˜ì§‘] {title[:45]}")

            except Exception as e:
                print(f"      - êµ­ì œêµë¥˜ì²˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        if stop_crawling:
            break

    print("âœ… [êµ­ì œêµë¥˜ì²˜] ì™„ë£Œ")

def crawl_sw(start_date_obj):
    print("\nğŸ“‚ [SWì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨] ì‹œì‘")
    base_url = "https://sw.kangwon.ac.kr"
    stop_crawling = False

    for page in tqdm(range(1, 100), desc="  [SWì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨]", leave=False):
        if stop_crawling:
            break

        list_url = f"{base_url}/index.php?mt=page&mp=5_1&mm=oxbbs&oxid=1&cpage={page}"
        soup = get_soup(list_url)
        if not soup:
            break

        rows = soup.select("table.bbs_list > tbody > tr")
        if not rows:
            break

        for row in rows:
            try:
                date_td = row.select_one("td:nth-last-child(2)")
                title_tag = row.select_one("td.tit a")

                if not date_td or not title_tag:
                    continue

                date = date_td.get_text(strip=True).replace("-", ".")
                is_notice_row = bool(row.select_one("img[alt='ê³µì§€ê¸€']"))

                if is_too_old(date, start_date_obj):
                    if is_notice_row:
                        continue
                    else:
                        print(f"      ğŸ”š [SWì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨] ì§€ì •ëœ ì‹œì‘ ë‚ ì§œ ì´ì „ ê²Œì‹œë¬¼ ë°œê²¬, ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        stop_crawling = True
                        break

                title = title_tag.get_text(strip=True)
                detail_url = urljoin(base_url, title_tag.get("href"))

                detail_soup = get_soup(detail_url)
                if not detail_soup:
                    continue
                delay_request()

                content_div = detail_soup.select_one("table.bbs_view td.bbs_td[colspan='6']")
                content = clean_html_keep_table(str(content_div)) if content_div else "(ë³¸ë¬¸ ì—†ìŒ)"

                img_files = []
                if content_div:
                    prefix = f"sw_{generate_notice_key(title, date)}"
                    folder_path = os.path.join(SAVE_FOLDER, "sw")

                    for i, img in enumerate(content_div.select("img")):
                        src = img.get("src")
                        if src and not src.startswith("data:"):
                            full_img_url = urljoin(detail_url, src)
                            saved_path = save_image(full_img_url, folder_path, prefix, i, src)
                            if saved_path:
                                img_files.append(saved_path)

                if add_notice_if_not_duplicate(title, date, content, detail_url, img_files):
                    print(f"      ğŸ“„ [ìˆ˜ì§‘] {title[:45]}")

            except Exception as e:
                print(f"      âŒ SWì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {title[:30]} ({e})")

        if stop_crawling:
            break

    print("âœ… [SWì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨] ì™„ë£Œ")

# --- ë°ì´í„° ì†ŒìŠ¤ ---
college_intro_pages = [
    {'college_name': 'ê°„í˜¸ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1782&'},
    {'college_name': 'ê²½ì˜ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1752&'},
    {'college_name': 'ë†ì—…ìƒëª…ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1758&'},
    {'college_name': 'ë™ë¬¼ìƒëª…ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1761&'},
    {'college_name': 'ë¬¸í™”ì˜ˆìˆ  ê³µê³¼ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1912&'},
    {'college_name': 'ì‚¬ë²”ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1767&'},
    {'college_name': 'ì‚¬íšŒê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1770&'},
    {'college_name': 'ì‚°ë¦¼í™˜ê²½ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1773&'},
    {'college_name': 'ìˆ˜ì˜ê³¼ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1776&'},
    {'college_name': 'ì•½í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1779&'},
    {'college_name': 'ì˜ê³¼ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1975&'},
    {'college_name': 'ì˜ìƒëª…ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1785&'},
    {'college_name': 'ì¸ë¬¸ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1788&'},
    {'college_name': 'ìì—°ê³¼í•™ëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1791&'},
    {'college_name': 'ITëŒ€í•™', 'url': 'https://wwwk.kangwon.ac.kr/www/contents.do?key=1794&'},
]
manual_board_mapping = {
    "AIìœµí•©í•™ê³¼": "https://ai.kangwon.ac.kr/ai/community/notice.do",
    "ë””ì§€í„¸ë°€ë¦¬í„°ë¦¬í•™ê³¼": "https://military.kangwon.ac.kr/military/professor/notice.do",
    "ììœ ì „ê³µí•™ë¶€": "https://liberal.kangwon.ac.kr/liberal/community/notice.do",
    "ê¸€ë¡œë²Œìœµí•©í•™ë¶€": "https://globalconvergence.kangwon.ac.kr/globalconvergence/info/undergraduate-community.do",
    "ë¯¸ë˜ìœµí•©ê°€ìƒí•™ê³¼": "https://multimajor.kangwon.ac.kr/multimajor/community/notice.do",
    "ë™ë¬¼ì‚°ì—…ìœµí•©í•™ê³¼": "https://animal.kangwon.ac.kr/animal/community/notice.do"
}

# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
if __name__ == "__main__":
    print("\nğŸš€ ê°•ì›ëŒ€ ì „ì²´ ê³µì§€ í¬ë¡¤ë§ ì‹œì‘")

    try:
        START_DATE_OBJ = datetime.strptime(CRAWL_START_DATE, "%Y-%m-%d")
        print(f"ğŸ—“ï¸  ìˆ˜ì§‘ ì‹œì‘ ë‚ ì§œ: {CRAWL_START_DATE} ì´í›„ì˜ ëª¨ë“  ìƒˆ ê²Œì‹œë¬¼ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    except ValueError:
        print(f"âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. 'YYYY-MM-DD' í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”. (ì…ë ¥ê°’: {CRAWL_START_DATE})")
        exit()

    load_existing_data()

    crawl_mainpage(START_DATE_OBJ)
    crawl_library(START_DATE_OBJ)
    crawl_engineering(START_DATE_OBJ)
    crawl_international(START_DATE_OBJ)
    crawl_sw(START_DATE_OBJ)

    boards = extract_notice_board_urls(college_intro_pages)
    if boards:
        crawl_all_departments(boards, START_DATE_OBJ, max_page=None)

    # --- ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥ (ë¡œì»¬ + Azure ì—…ë¡œë“œ) ---
    if not all_data:
        print("\nâœ… ì¶”ê°€í•  ìƒˆë¡œìš´ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"\nğŸ”„ ì´ {len(all_data)}ê±´ì˜ ìƒˆë¡œìš´ ê²Œì‹œê¸€ì„ ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤...")

        try:
            # 1) ê¸°ì¡´ ë°ì´í„° + ì‹ ê·œ ë°ì´í„° ë³‘í•©
            final_data_to_save = pre_existing_data + all_data

            # 2) CSV ë¬¸ìì—´ë¡œ ë³€í™˜ (UTF-8-SIG)
            output_stream = io.StringIO()
            writer = csv.DictWriter(output_stream, fieldnames=["ì œëª©", "ì‘ì„±ì¼", "ë³¸ë¬¸ë‚´ìš©", "ë§í¬", "ì‚¬ì§„"])
            writer.writeheader()
            writer.writerows(final_data_to_save)
            csv_output_string = output_stream.getvalue()

            # 3) âœ… ë¨¼ì € "ë¡œì»¬"ì— ì €ì¥
            os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)
            with open(CSV_PATH, "w", newline="", encoding="utf-8-sig") as f:
                f.write(csv_output_string)
            print(f"ğŸ’¾ ë¡œì»¬ ì €ì¥ ì™„ë£Œ: {CSV_PATH} (ì´ {len(final_data_to_save)}ê±´)")

            # 4) ê·¸ ë‹¤ìŒ Azure Blobì—ë„ ì—…ë¡œë“œ(ê°€ëŠ¥í•  ë•Œ)
            if not blob_service_client:
                print("âš ï¸ Azure í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ Blob ì—…ë¡œë“œëŠ” ê±´ë„ˆëœë‹ˆë‹¤.")
            else:
                try:
                    blob_client = blob_service_client.get_blob_client(container=AZURE_CSV_CONTAINER_NAME, blob=CSV_FILE)
                    blob_client.upload_blob(csv_output_string.encode("utf-8-sig"), overwrite=True)
                    print(f"â˜ï¸ Azure Storage ì—…ë¡œë“œ ì™„ë£Œ: container='{AZURE_CSV_CONTAINER_NAME}', blob='{CSV_FILE}'")
                except Exception as e:
                    print(f"âš ï¸ Blob ì—…ë¡œë“œ ì‹¤íŒ¨(ë¡œì»¬ ì €ì¥ì€ ì™„ë£Œ): {e}")

            print("âœ… ì €ì¥ ì ˆì°¨ ì™„ë£Œ (ë¡œì»¬ â†’ Blob ìˆœì„œ)")

        except Exception as e:
            print(f"âŒ CSV ì €ì¥/ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
