from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import time
import csv
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

import os
import csv
import time
import requests
from hashlib import md5
from urllib.parse import urlparse

# ChromeDriver ê²½ë¡œ ì„¤ì •
CHROMEDRIVER_PATH = "C:/Users/YOOJIIN/Downloads/chromedriver-win64/chromedriver-win64/chromedriver.exe" 

# Chrome ì‹¤í–‰ ì˜µì…˜
options = Options()
# options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì•ˆ ë„ìš°ê³  ì‹¤í–‰ (ì›í•˜ë©´ ì£¼ì„ì²˜ë¦¬í•´ë„ ë¨)
options.add_argument("--disable-gpu")
options.add_argument("--no-sandbox")

# Selenium WebDriver ì‹¤í–‰
service = Service(CHROMEDRIVER_PATH)
driver = webdriver.Chrome(service=service, options=options)

# í¬ë¡¤ë§í•  URL
base_url = "https://library.kangwon.ac.kr"

image_dir = os.path.join("data", "images")
os.makedirs(image_dir, exist_ok=True)

# CSV ì €ì¥ìš©
results = []

# ì „ì²´ ê³µì§€ ê°œìˆ˜ ê¸°ë°˜ offset ë¦¬ìŠ¤íŠ¸ ìƒì„±
driver.get(f"{base_url}/community/bulletin/notice?max=100&offset=0&bulletinCategoryId=1")
time.sleep(3)
soup = BeautifulSoup(driver.page_source, "html.parser")
total_text = soup.select_one("span.ikc-active")
total_count = int(total_text.get_text(strip=True)) if total_text else 0
offset_list = list(range(0, total_count, 100))
print(f"ì´ {total_count}ê°œì˜ ê³µì§€ì‚¬í•­ ë°œê²¬, {len(offset_list)}í˜ì´ì§€ ìˆœíšŒ ì˜ˆì •")

# ğŸ” í˜ì´ì§€ë„¤ì´ì…˜ (0, 20, 40, ... ìµœëŒ€ 100ê¹Œì§€ ì‹œë„)
for offset in offset_list:
    list_url = f"{base_url}/community/bulletin/notice?max=100&offset={offset}&bulletinCategoryId=1"
    print(f"\nğŸ“„ [í˜ì´ì§€ offset={offset}] í¬ë¡¤ë§ ì¤‘...")
    driver.get(list_url)
    time.sleep(3)

    items = driver.find_elements(By.CSS_SELECTOR, "a.ikc-bulletins-title")
    total = len(items)
    print(f"{total}ê°œ ê³µì§€ ë°œê²¬")

    for i in range(total):
        items = driver.find_elements(By.CSS_SELECTOR, "a.ikc-bulletins-title")

        if i >= len(items):
            print(f"í•­ëª© ëˆ„ë½ ê°ì§€ (i={i}, items ìˆ˜={len(items)}), ê±´ë„ˆëœ€")
            continue

        item = items[i]
        title = item.text.strip()
        print(f"\n! {i+1}. {title}")

        try:
            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", item)
            time.sleep(1)
            
            item.click()

            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "ikc-bulletin-content"))
            )

            detail_soup = BeautifulSoup(driver.page_source, "html.parser")
            content_div = detail_soup.select_one("div.ikc-bulletin-content")

            if content_div:
                paragraphs = content_div.find_all("p")
                body = "\n".join(p.get_text(strip=True) for p in paragraphs)
            else:
                body = "ë³¸ë¬¸ ì—†ìŒ"

            # ì‘ì„±ì¼ ì¶”ì¶œ
            date = "ì‘ì„±ì¼ ì •ë³´ ì—†ìŒ"
            for li in detail_soup.select("li"):
                label = li.select_one("label")
                if label and "ì‘ì„±ì¼" in label.text:
                    span = li.select_one("span")
                    if span:
                        date = span.text.strip()
                        break

            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            img_urls = []
            for img in content_div.find_all("img"):
                src = img.get("src")
                if src:
                    src = src.strip().strip('"')  # ì•ë’¤ ê³µë°±ê³¼ ë”°ì˜´í‘œ ì œê±°
                    if src.startswith("http"):   # ì ˆëŒ€ê²½ë¡œ
                        img_url = src
                    elif src.startswith("//"):
                        img_url = "https:" + src
                    else:                        # ìƒëŒ€ê²½ë¡œ
                        img_url = base_url + src
                    img_urls.append(img_url)

            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            saved_filenames = []
            for url in img_urls:
                try:
                    ext = os.path.splitext(urlparse(url).path)[1]
                    if not ext or len(ext) > 5:
                        ext = ".jpg"  # ê¸°ë³¸ í™•ì¥ì ì„¤ì •
                    
                    filename = md5(url.encode()).hexdigest() + ext
                    filepath = os.path.join(image_dir, filename)

                    r = requests.get(url, timeout=10)
                    with open(filepath, "wb") as f:
                        f.write(r.content)
                    # CSVì—ëŠ” ìƒëŒ€ê²½ë¡œë¡œ ì €ì¥
                    saved_filenames.append(os.path.join(image_dir, filename).replace("\\", "/"))
                except Exception as e:
                    print(f"ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {url} â†’ {e}")

            
            # ë§í¬ëŠ” í˜„ì¬ í˜ì´ì§€ URL
            detail_url = driver.current_url

            # ê²°ê³¼ ì €ì¥
            results.append([title, date, body, detail_url, ";".join(saved_filenames)])

            driver.back()
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "a.ikc-bulletins-title"))
            )
            itmes = driver.find_element(By.CSS_SELECTOR, "a.ikc-bulletins-title")

        except Exception as e:
            print(f"ë³¸ë¬¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            results.append([title, "ë³¸ë¬¸ ë¡œë”© ì‹¤íŒ¨"])

driver.quit()

# CSVë¡œ ì €ì¥
csv_path = os.path.join("data", "kangwon_library_notice.csv")
with open(csv_path, "w", newline="", encoding="utf-8-sig") as f:
    writer = csv.writer(f)
    writer.writerow(["ì œëª©", "ì‘ì„±ì¼", "ë³¸ë¬¸", "ìƒì„¸ ë§í¬", "ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¡œ"])
    writer.writerows(results)

print(f"\nâœ… ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! ğŸ“ '{csv_path}' íŒŒì¼ë¡œ ì €ì¥ë¨.")
