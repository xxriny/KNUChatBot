import requests
from bs4 import BeautifulSoup
import os
import csv
import time
import base64
from urllib.parse import urljoin
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import mysql.connector
import re
from datetime import datetime

BASE_URL = 'https://icee.kangwon.ac.kr'
LIST_URL_TEMPLATE = BASE_URL + '/index.php?mt=page&mp=5_1&mm=oxbbs&oxid=1&cpage={}'
SAVE_FOLDER = '../../data/images'
CSV_FOLDER = '../../data'
CSV_FILE = os.path.join(CSV_FOLDER, 'icee_crawl.csv')
HEADERS = {"User-Agent": "Mozilla/5.0"}

session = requests.Session()
retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
adapter = HTTPAdapter(max_retries=retries)
session.mount('http://', adapter)
session.mount('https://', adapter)

os.makedirs(SAVE_FOLDER, exist_ok=True)
os.makedirs(CSV_FOLDER, exist_ok=True)

# db
db = mysql.connector.connect(
    host='localhost',
    user='root',        # ğŸ” ì‚¬ìš©ì ì„¤ì •
    password='1234',    # ğŸ” ë¹„ë°€ë²ˆí˜¸ ì„¤ì •
    database='icee_crawl'
)
cursor = db.cursor()

def parse_date(date_str):
    if re.match(r'\d{4}-\d{2}-\d{2}', date_str):
        return date_str
    return None

with open(CSV_FILE, 'w', newline='', encoding='utf-8-sig') as f:
    writer = csv.writer(f)
    writer.writerow(['ê²Œì‹œíŒì¢…ë¥˜', 'ì œëª©', 'ì‘ì„±ì¼', 'ë³¸ë¬¸ë‚´ìš©', 'ë§í¬', 'ì‚¬ì§„'])

    page = 1
    while True:
        print(f'\nğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...')
        res = requests.get(LIST_URL_TEMPLATE.format(page), headers=HEADERS)
        if res.status_code != 200:
            print(f'âŒ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {res.status_code}')
            break

        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.select('table.bbs_list tbody tr')
        if not rows:
            print("âŒ ë” ì´ìƒ ê²Œì‹œê¸€ ì—†ìŒ. ì¢…ë£Œ.")
            break

        for row in rows:
            try:
                a_tag = row.select_one('td.tit a')
                if not a_tag:
                    continue

                post_url = urljoin(BASE_URL, a_tag['href'])
                title = a_tag.text.strip()
                date_raw = row.select_one('td.dt').text.strip()
                post_date = parse_date(date_raw)

                post_res = requests.get(post_url, headers=HEADERS)
                if post_res.status_code != 200:
                    continue

                post_soup = BeautifulSoup(post_res.text, 'html.parser')
                content_tag = post_soup.select_one('div.view_cont') or post_soup.select_one('div.note')
                content = content_tag.get_text(separator='\n', strip=True) if content_tag else 'ë³¸ë¬¸ ì—†ìŒ'

                # ì´ë¯¸ì§€ ì €ì¥
                img_filenames = []
                if content_tag:
                    for idx, img in enumerate(content_tag.find_all('img')):
                        img_src = img.get('src')
                        if not img_src:
                            continue

                        if img_src.startswith('data:image'):
                            try:
                                header, b64data = img_src.split(',', 1)
                                ext = header.split('/')[1].split(';')[0]
                                img_name = f"base64_{page}_{idx}.{ext}"
                                save_path = os.path.join(SAVE_FOLDER, img_name)
                                with open(save_path, 'wb') as f_img:
                                    f_img.write(base64.b64decode(b64data))
                                img_filenames.append(os.path.join(SAVE_FOLDER, img_name).replace('\\', '/'))
                                time.sleep(0.2)
                            except Exception as e:
                                print(f'âš ï¸ Base64 ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}')
                                continue
                        else:
                            try:
                                img_url = urljoin(post_url, img_src)
                                img_name = os.path.basename(img_url)
                                save_path = os.path.join(SAVE_FOLDER, img_name)
                                img_res = session.get(img_url, headers=HEADERS, stream=True, timeout=10)
                                img_res.raise_for_status()
                                with open(save_path, 'wb') as f_img:
                                    for chunk in img_res.iter_content(1024):
                                        f_img.write(chunk)
                                img_filenames.append(os.path.join(SAVE_FOLDER, img_name).replace('\\', '/'))
                                time.sleep(0.2)
                            except Exception as e:
                                print(f'âš ï¸ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {img_src} ({e})')
                                continue

                # CSV ì €ì¥
                writer.writerow(['ê³µì§€ì‚¬í•­', title, f"'{date_raw}", content, post_url, ';'.join(img_filenames)])

                # DB ì €ì¥
                try:
                    sql = "INSERT INTO posts (board_type, title, post_date, content, link, image_files) VALUES (%s, %s, %s, %s, %s, %s)"
                    cursor.execute(sql, (
                        'ê³µì§€ì‚¬í•­',
                        title,
                        post_date,  
                        content,
                        post_url,
                        ';'.join(img_filenames)
                    ))
                    db.commit()
                except Exception as db_error:
                    print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {db_error}")

                print(f'âœ… ì €ì¥ë¨: {title[:50]}...')
                time.sleep(1)

            except Exception as e:
                print(f'âŒ ê²Œì‹œê¸€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}')
                continue

        # ë‹¤ìŒ í˜ì´ì§€ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if not soup.select_one(f'a[href*="cpage={page+1}"]'):
            print("âœ… ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬")
            break

        page += 1

cursor.close()
db.close()
print('\nğŸ‰ ì „ì²´ í¬ë¡¤ë§ ë° ì €ì¥ ì™„ë£Œ!')


# import requests
# from bs4 import BeautifulSoup
# import os
# import csv
# import time
# import base64
# from urllib.parse import urljoin
# from requests.adapters import HTTPAdapter
# from urllib3.util.retry import Retry

# # ì„¤ì •
# BASE_URL = 'https://icee.kangwon.ac.kr'
# LIST_URL_TEMPLATE = BASE_URL + '/index.php?mt=page&mp=5_1&mm=oxbbs&oxid=1&cpage={}'
# SAVE_FOLDER = 'images'
# CSV_FOLDER = '../scripts/crawl'
# CSV_FILE = os.path.join(CSV_FOLDER,'icee_crawl.csv')
# HEADERS = {"User-Agent": "Mozilla/5.0"}

# # ì´ë¯¸ì§€ ì¬ì‹œë„ ì„¤ì •
# session = requests.Session()
# retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
# adapter = HTTPAdapter(max_retries=retries)
# session.mount('http://', adapter)
# session.mount('https://', adapter)

# # í´ë” ìƒì„±
# if not os.path.exists(SAVE_FOLDER):
#     os.makedirs(SAVE_FOLDER)

# # CSV ì €ì¥
# with open(CSV_FILE, 'w', newline='', encoding='utf-8-sig') as f:
#     writer = csv.writer(f)
#     writer.writerow(['ê²Œì‹œíŒì¢…ë¥˜', 'ì œëª©', 'ì‘ì„±ì¼', 'ë³¸ë¬¸ë‚´ìš©', 'ë§í¬', 'ì‚¬ì§„'])

#     page = 1
#     while True:
#         print(f'\nğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...')
#         res = requests.get(LIST_URL_TEMPLATE.format(page), headers=HEADERS)
#         if res.status_code != 200:
#             print(f'âŒ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {res.status_code}')
#             break

#         soup = BeautifulSoup(res.text, 'html.parser')
#         rows = soup.select('table.bbs_list tbody tr')
#         if not rows:
#             print("âŒ ë” ì´ìƒ ê²Œì‹œê¸€ ì—†ìŒ. ì¢…ë£Œ.")
#             break

#         for row in rows:
#             try:
#                 a_tag = row.select_one('td.tit a')
#                 if not a_tag:
#                     continue

#                 post_url = urljoin(BASE_URL, a_tag['href'])
#                 title = a_tag.text.strip()
#                 date = row.select_one('td.dt').text.strip()

#                 post_res = requests.get(post_url, headers=HEADERS)
#                 if post_res.status_code != 200:
#                     continue

#                 post_soup = BeautifulSoup(post_res.text, 'html.parser')
#                 content_tag = post_soup.select_one('div.view_cont') or post_soup.select_one('div.note')
#                 content = content_tag.get_text(separator='\n', strip=True) if content_tag else 'ë³¸ë¬¸ ì—†ìŒ'

#                 # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (URL ë˜ëŠ” base64 ë‘˜ ë‹¤ ì²˜ë¦¬)
#                 img_filenames = []
#                 if content_tag:
#                     for idx, img in enumerate(content_tag.find_all('img')):
#                         img_src = img.get('src')
#                         if not img_src:
#                             continue

#                         if img_src.startswith('data:image'):
#                             try:
#                                 header, b64data = img_src.split(',', 1)
#                                 ext = header.split('/')[1].split(';')[0]
#                                 img_name = f"base64_{page}_{idx}.{ext}"
#                                 save_path = os.path.join(SAVE_FOLDER, img_name)

#                                 with open(save_path, 'wb') as f_img:
#                                     f_img.write(base64.b64decode(b64data))
#                                 img_filenames.append(img_name)
#                                 time.sleep(0.2)
#                             except Exception as e:
#                                 print(f'âš ï¸ Base64 ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}')
#                                 continue
#                         else:
#                             try:
#                                 img_url = urljoin(post_url, img_src)
#                                 img_name = os.path.basename(img_url)
#                                 save_path = os.path.join(SAVE_FOLDER, img_name)

#                                 img_res = session.get(img_url, headers=HEADERS, stream=True, timeout=10)
#                                 img_res.raise_for_status()

#                                 with open(save_path, 'wb') as f_img:
#                                     for chunk in img_res.iter_content(1024):
#                                         f_img.write(chunk)
#                                 img_filenames.append(img_name)
#                                 time.sleep(0.2)
#                             except Exception as e:
#                                 print(f'âš ï¸ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {img_src} ({e})')
#                                 continue

#                 writer.writerow([
#                     'ê³µì§€ì‚¬í•­',
#                     title,
#                     f"'{date}",
#                     content,
#                     post_url,
#                     ';'.join(img_filenames)
#                 ])
#                 print(f'âœ… ì €ì¥ë¨: {title[:50]}...')
#                 time.sleep(1)

#             except Exception as e:
#                 print(f'âŒ ê²Œì‹œê¸€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}')
#                 continue

#         if not soup.select_one(f'a[href*="cpage={page+1}"]'):
#             print("âœ… ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬")
#             break

#         page += 1

# print('\n ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ!')
